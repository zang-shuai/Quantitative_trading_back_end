{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# myseed = 42069\n",
    "myseed = 4\n",
    "np.random.seed(myseed)\n",
    "# 保持可复现性：\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 因此方便复现、提升训练速度就：\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 工具包"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, 5.)\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
    "    ''' Plot prediction of your DNN '''\n",
    "    if preds is None or targets is None:\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        for x, y in dv_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                preds.append(pred.detach().cpu())\n",
    "                targets.append(y.detach().cpu())\n",
    "        preds = torch.cat(preds, dim=0).numpy()\n",
    "        targets = torch.cat(targets, dim=0).numpy()\n",
    "\n",
    "    figure(figsize=(5, 5))\n",
    "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
    "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
    "    plt.xlim(-0.2, lim)\n",
    "    plt.ylim(-0.2, lim)\n",
    "    plt.xlabel('ground truth value')\n",
    "    plt.ylabel('predicted value')\n",
    "    plt.title('Ground Truth v.s. Prediction')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 设置数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def swap(target):\n",
    "    ans = []\n",
    "    for i in target:\n",
    "        a = np.zeros(10)\n",
    "        a[i] = 1\n",
    "        ans.append(a)\n",
    "    return ans\n",
    "\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, path, mode='train', target_only=False):\n",
    "        self.mode = mode\n",
    "        data = pd.read_csv(path, header=None)\n",
    "        data = np.array(data)\n",
    "        feats = []\n",
    "        if not target_only:\n",
    "            feats = []\n",
    "\n",
    "        if mode == 'test':\n",
    "            self.data = torch.FloatTensor(data[:, 1:])\n",
    "        else:\n",
    "            target = data[:, 0]\n",
    "            data = data[:, 1:]\n",
    "            indices = []\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "\n",
    "            self.data = torch.Tensor(data[indices] / 255)\n",
    "\n",
    "            self.target = torch.Tensor(swap(target[indices]))\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "        print('Finished reading the {} set of mnist Dataset ({} samples found, each dim = {})'\n",
    "              .format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            return self.data[item], self.target[item]\n",
    "        else:\n",
    "            return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据加载"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "    dataset = MnistDataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle=(mode == 'train'), drop_last=False,\n",
    "        num_workers=n_jobs, pin_memory=True)  # Construct dataloader\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 神经网络"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Define your neural network here\n",
    "        # TODO: How to modify this model to achieve better performance?\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        # Mean squared error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        # TODO: you may implement L2 regularization here\n",
    "        return self.criterion(pred, target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def train(tr_set, dv_set, model, config, device):\n",
    "    ''' DNN training '''\n",
    "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
    "    # Setup optimizer\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])\n",
    "\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}  # for recording training loss\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()  # set model to training mode\n",
    "        for x, y in tr_set:  # iterate through the dataloader\n",
    "            optimizer.zero_grad()  # set gradient to zero\n",
    "            x, y = x.to(device), y.to(device)  # move data to device (cpu/cuda)\n",
    "            pred = model(x)  # forward pass (compute output)\n",
    "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
    "            mse_loss.backward()  # compute gradient (backpropagation)\n",
    "            optimizer.step()  # update model with optimizer\n",
    "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
    "\n",
    "        # After each epoch, test your model on the validation (development) set.\n",
    "        dev_mse = dev(dv_set, model, device)\n",
    "        if dev_mse < min_mse:\n",
    "            # Save model if your model improved\n",
    "            min_mse = dev_mse\n",
    "            print('Saving model (epoch = {:4d}, loss = {:.4f})'.format(epoch + 1, min_mse))\n",
    "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "\n",
    "        epoch += 1\n",
    "        loss_record['dev'].append(dev_mse)\n",
    "        if early_stop_cnt > config['early_stop']:\n",
    "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
    "            break\n",
    "\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return min_mse, loss_record"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 验证函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def dev(dv_set, model, device):\n",
    "    model.eval()  # set model to evalutation mode\n",
    "    total_loss = 0\n",
    "    for x, y in dv_set:  # iterate through the dataloader\n",
    "        x, y = x.to(device), y.to(device)  # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():  # disable gradient calculation\n",
    "            pred = model(x)  # forward pass (compute output)\n",
    "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
    "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
    "    total_loss = total_loss / len(dv_set.dataset)  # compute averaged loss\n",
    "\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 测试函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def test(tt_set, model, device):\n",
    "    model.eval()  # set model to evalutation mode\n",
    "    preds = []\n",
    "    for x in tt_set:  # iterate through the dataloader\n",
    "        x = x.to(device)  # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():  # disable gradient calculation\n",
    "            pred = model(x)  # forward pass (compute output)\n",
    "            preds.append(pred.detach().cpu())  # collect prediction\n",
    "    preds = torch.cat(preds, dim=0).numpy()  # concatenate all predictions and convert to a numpy array\n",
    "    return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 设置超参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.optim.sgd.SGD"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()  # get the current available device ('cpu' or 'cuda')\n",
    "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
    "target_only = False  # TODO: Using 40 states & 2 tested_positive features\n",
    "\n",
    "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
    "config = {\n",
    "    # 总共训练多少次\n",
    "    'n_epochs': 3000,  # maximum number of epochs\n",
    "    'batch_size': 270,  # mini-batch size for dataloader\n",
    "    'optimizer': 'SGD',  # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {  # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': 0.001,  # learning rate of SGD\n",
    "        'momentum': 0.9  # momentum for SGD\n",
    "    },\n",
    "    'early_stop': 200,  # early stopping epochs (the number epochs since your model's last improvement)\n",
    "    'save_path': '../models/model.pth'  # your model will be saved here\n",
    "}\n",
    "getattr(torch.optim, config['optimizer'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of mnist Dataset (54000 samples found, each dim = 784)\n",
      "Finished reading the dev set of mnist Dataset (6000 samples found, each dim = 784)\n",
      "Finished reading the test set of mnist Dataset (10000 samples found, each dim = 784)\n"
     ]
    }
   ],
   "source": [
    "tr_path = '../data/mnist_train.csv'  # path to training data\n",
    "tt_path = '../data/mnist_test.csv'  # path to testing data\n",
    "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
    "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
    "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model (epoch =    1, loss = 0.0853)\n",
      "Saving model (epoch =    2, loss = 0.0786)\n",
      "Saving model (epoch =    3, loss = 0.0730)\n",
      "Saving model (epoch =    4, loss = 0.0682)\n",
      "Saving model (epoch =    5, loss = 0.0643)\n",
      "Saving model (epoch =    6, loss = 0.0612)\n",
      "Saving model (epoch =    7, loss = 0.0587)\n",
      "Saving model (epoch =    8, loss = 0.0567)\n",
      "Saving model (epoch =    9, loss = 0.0550)\n",
      "Saving model (epoch =   10, loss = 0.0535)\n",
      "Saving model (epoch =   11, loss = 0.0522)\n",
      "Saving model (epoch =   12, loss = 0.0511)\n",
      "Saving model (epoch =   13, loss = 0.0501)\n",
      "Saving model (epoch =   14, loss = 0.0492)\n",
      "Saving model (epoch =   15, loss = 0.0483)\n",
      "Saving model (epoch =   16, loss = 0.0476)\n",
      "Saving model (epoch =   17, loss = 0.0469)\n",
      "Saving model (epoch =   18, loss = 0.0462)\n",
      "Saving model (epoch =   19, loss = 0.0456)\n",
      "Saving model (epoch =   20, loss = 0.0451)\n",
      "Saving model (epoch =   21, loss = 0.0446)\n",
      "Saving model (epoch =   22, loss = 0.0441)\n",
      "Saving model (epoch =   23, loss = 0.0436)\n",
      "Saving model (epoch =   24, loss = 0.0432)\n",
      "Saving model (epoch =   25, loss = 0.0427)\n",
      "Saving model (epoch =   26, loss = 0.0423)\n",
      "Saving model (epoch =   27, loss = 0.0419)\n",
      "Saving model (epoch =   28, loss = 0.0415)\n",
      "Saving model (epoch =   29, loss = 0.0412)\n",
      "Saving model (epoch =   30, loss = 0.0408)\n",
      "Saving model (epoch =   31, loss = 0.0405)\n",
      "Saving model (epoch =   32, loss = 0.0401)\n",
      "Saving model (epoch =   33, loss = 0.0398)\n",
      "Saving model (epoch =   34, loss = 0.0395)\n",
      "Saving model (epoch =   35, loss = 0.0391)\n",
      "Saving model (epoch =   36, loss = 0.0388)\n",
      "Saving model (epoch =   37, loss = 0.0385)\n",
      "Saving model (epoch =   38, loss = 0.0382)\n",
      "Saving model (epoch =   39, loss = 0.0379)\n",
      "Saving model (epoch =   40, loss = 0.0377)\n",
      "Saving model (epoch =   41, loss = 0.0374)\n",
      "Saving model (epoch =   42, loss = 0.0371)\n",
      "Saving model (epoch =   43, loss = 0.0368)\n",
      "Saving model (epoch =   44, loss = 0.0366)\n",
      "Saving model (epoch =   45, loss = 0.0363)\n",
      "Saving model (epoch =   46, loss = 0.0361)\n",
      "Saving model (epoch =   47, loss = 0.0358)\n",
      "Saving model (epoch =   48, loss = 0.0356)\n",
      "Saving model (epoch =   49, loss = 0.0353)\n",
      "Saving model (epoch =   50, loss = 0.0351)\n",
      "Saving model (epoch =   51, loss = 0.0349)\n",
      "Saving model (epoch =   52, loss = 0.0346)\n",
      "Saving model (epoch =   53, loss = 0.0344)\n",
      "Saving model (epoch =   54, loss = 0.0342)\n",
      "Saving model (epoch =   55, loss = 0.0340)\n",
      "Saving model (epoch =   56, loss = 0.0338)\n",
      "Saving model (epoch =   57, loss = 0.0336)\n",
      "Saving model (epoch =   58, loss = 0.0334)\n",
      "Saving model (epoch =   59, loss = 0.0332)\n",
      "Saving model (epoch =   60, loss = 0.0330)\n",
      "Saving model (epoch =   61, loss = 0.0328)\n",
      "Saving model (epoch =   62, loss = 0.0326)\n",
      "Saving model (epoch =   63, loss = 0.0324)\n",
      "Saving model (epoch =   64, loss = 0.0323)\n",
      "Saving model (epoch =   65, loss = 0.0321)\n",
      "Saving model (epoch =   66, loss = 0.0319)\n",
      "Saving model (epoch =   67, loss = 0.0318)\n",
      "Saving model (epoch =   68, loss = 0.0316)\n",
      "Saving model (epoch =   69, loss = 0.0314)\n",
      "Saving model (epoch =   70, loss = 0.0313)\n",
      "Saving model (epoch =   71, loss = 0.0311)\n",
      "Saving model (epoch =   72, loss = 0.0310)\n",
      "Saving model (epoch =   73, loss = 0.0308)\n",
      "Saving model (epoch =   74, loss = 0.0307)\n",
      "Saving model (epoch =   75, loss = 0.0305)\n",
      "Saving model (epoch =   76, loss = 0.0304)\n",
      "Saving model (epoch =   77, loss = 0.0302)\n",
      "Saving model (epoch =   78, loss = 0.0301)\n",
      "Saving model (epoch =   79, loss = 0.0300)\n",
      "Saving model (epoch =   80, loss = 0.0298)\n",
      "Saving model (epoch =   81, loss = 0.0297)\n",
      "Saving model (epoch =   82, loss = 0.0296)\n",
      "Saving model (epoch =   83, loss = 0.0294)\n",
      "Saving model (epoch =   84, loss = 0.0293)\n",
      "Saving model (epoch =   85, loss = 0.0292)\n",
      "Saving model (epoch =   86, loss = 0.0291)\n",
      "Saving model (epoch =   87, loss = 0.0290)\n",
      "Saving model (epoch =   88, loss = 0.0288)\n",
      "Saving model (epoch =   89, loss = 0.0287)\n",
      "Saving model (epoch =   90, loss = 0.0286)\n",
      "Saving model (epoch =   91, loss = 0.0285)\n",
      "Saving model (epoch =   92, loss = 0.0284)\n",
      "Saving model (epoch =   93, loss = 0.0283)\n",
      "Saving model (epoch =   94, loss = 0.0282)\n",
      "Saving model (epoch =   95, loss = 0.0281)\n",
      "Saving model (epoch =   96, loss = 0.0280)\n",
      "Saving model (epoch =   97, loss = 0.0279)\n",
      "Saving model (epoch =   98, loss = 0.0278)\n",
      "Saving model (epoch =   99, loss = 0.0277)\n",
      "Saving model (epoch =  100, loss = 0.0276)\n",
      "Saving model (epoch =  101, loss = 0.0275)\n",
      "Saving model (epoch =  102, loss = 0.0274)\n",
      "Saving model (epoch =  103, loss = 0.0273)\n",
      "Saving model (epoch =  104, loss = 0.0273)\n",
      "Saving model (epoch =  105, loss = 0.0272)\n",
      "Saving model (epoch =  106, loss = 0.0271)\n",
      "Saving model (epoch =  107, loss = 0.0270)\n",
      "Saving model (epoch =  108, loss = 0.0269)\n",
      "Saving model (epoch =  109, loss = 0.0268)\n",
      "Saving model (epoch =  110, loss = 0.0268)\n",
      "Saving model (epoch =  111, loss = 0.0267)\n",
      "Saving model (epoch =  112, loss = 0.0266)\n",
      "Saving model (epoch =  113, loss = 0.0265)\n",
      "Saving model (epoch =  114, loss = 0.0265)\n",
      "Saving model (epoch =  115, loss = 0.0264)\n",
      "Saving model (epoch =  116, loss = 0.0263)\n",
      "Saving model (epoch =  117, loss = 0.0262)\n",
      "Saving model (epoch =  118, loss = 0.0262)\n",
      "Saving model (epoch =  119, loss = 0.0261)\n",
      "Saving model (epoch =  120, loss = 0.0260)\n",
      "Saving model (epoch =  121, loss = 0.0260)\n",
      "Saving model (epoch =  122, loss = 0.0259)\n",
      "Saving model (epoch =  123, loss = 0.0259)\n",
      "Saving model (epoch =  124, loss = 0.0258)\n",
      "Saving model (epoch =  125, loss = 0.0257)\n",
      "Saving model (epoch =  126, loss = 0.0257)\n",
      "Saving model (epoch =  127, loss = 0.0256)\n",
      "Saving model (epoch =  128, loss = 0.0255)\n",
      "Saving model (epoch =  129, loss = 0.0255)\n",
      "Saving model (epoch =  130, loss = 0.0254)\n",
      "Saving model (epoch =  131, loss = 0.0254)\n",
      "Saving model (epoch =  132, loss = 0.0253)\n",
      "Saving model (epoch =  133, loss = 0.0253)\n",
      "Saving model (epoch =  134, loss = 0.0252)\n",
      "Saving model (epoch =  135, loss = 0.0251)\n",
      "Saving model (epoch =  136, loss = 0.0251)\n",
      "Saving model (epoch =  137, loss = 0.0250)\n",
      "Saving model (epoch =  138, loss = 0.0250)\n",
      "Saving model (epoch =  139, loss = 0.0249)\n",
      "Saving model (epoch =  140, loss = 0.0249)\n",
      "Saving model (epoch =  141, loss = 0.0248)\n",
      "Saving model (epoch =  142, loss = 0.0248)\n",
      "Saving model (epoch =  143, loss = 0.0247)\n",
      "Saving model (epoch =  144, loss = 0.0247)\n",
      "Saving model (epoch =  145, loss = 0.0246)\n",
      "Saving model (epoch =  146, loss = 0.0246)\n",
      "Saving model (epoch =  147, loss = 0.0245)\n",
      "Saving model (epoch =  148, loss = 0.0245)\n",
      "Saving model (epoch =  149, loss = 0.0244)\n",
      "Saving model (epoch =  150, loss = 0.0244)\n",
      "Saving model (epoch =  151, loss = 0.0243)\n",
      "Saving model (epoch =  152, loss = 0.0243)\n",
      "Saving model (epoch =  153, loss = 0.0242)\n",
      "Saving model (epoch =  154, loss = 0.0242)\n",
      "Saving model (epoch =  155, loss = 0.0242)\n",
      "Saving model (epoch =  156, loss = 0.0241)\n",
      "Saving model (epoch =  157, loss = 0.0241)\n",
      "Saving model (epoch =  158, loss = 0.0240)\n",
      "Saving model (epoch =  159, loss = 0.0240)\n",
      "Saving model (epoch =  160, loss = 0.0239)\n",
      "Saving model (epoch =  161, loss = 0.0239)\n",
      "Saving model (epoch =  162, loss = 0.0239)\n",
      "Saving model (epoch =  163, loss = 0.0238)\n",
      "Saving model (epoch =  164, loss = 0.0238)\n",
      "Saving model (epoch =  165, loss = 0.0237)\n",
      "Saving model (epoch =  166, loss = 0.0237)\n",
      "Saving model (epoch =  167, loss = 0.0237)\n",
      "Saving model (epoch =  168, loss = 0.0236)\n",
      "Saving model (epoch =  169, loss = 0.0236)\n",
      "Saving model (epoch =  170, loss = 0.0236)\n",
      "Saving model (epoch =  171, loss = 0.0235)\n",
      "Saving model (epoch =  172, loss = 0.0235)\n",
      "Saving model (epoch =  173, loss = 0.0234)\n",
      "Saving model (epoch =  174, loss = 0.0234)\n",
      "Saving model (epoch =  175, loss = 0.0234)\n",
      "Saving model (epoch =  176, loss = 0.0233)\n",
      "Saving model (epoch =  177, loss = 0.0233)\n",
      "Saving model (epoch =  178, loss = 0.0233)\n",
      "Saving model (epoch =  179, loss = 0.0232)\n",
      "Saving model (epoch =  180, loss = 0.0232)\n",
      "Saving model (epoch =  181, loss = 0.0232)\n",
      "Saving model (epoch =  182, loss = 0.0231)\n",
      "Saving model (epoch =  183, loss = 0.0231)\n",
      "Saving model (epoch =  184, loss = 0.0231)\n",
      "Saving model (epoch =  185, loss = 0.0230)\n",
      "Saving model (epoch =  186, loss = 0.0230)\n",
      "Saving model (epoch =  187, loss = 0.0230)\n",
      "Saving model (epoch =  188, loss = 0.0229)\n",
      "Saving model (epoch =  189, loss = 0.0229)\n",
      "Saving model (epoch =  190, loss = 0.0229)\n",
      "Saving model (epoch =  191, loss = 0.0228)\n",
      "Saving model (epoch =  192, loss = 0.0228)\n",
      "Saving model (epoch =  193, loss = 0.0228)\n",
      "Saving model (epoch =  194, loss = 0.0227)\n",
      "Saving model (epoch =  195, loss = 0.0227)\n",
      "Saving model (epoch =  196, loss = 0.0227)\n",
      "Saving model (epoch =  197, loss = 0.0226)\n",
      "Saving model (epoch =  198, loss = 0.0226)\n",
      "Saving model (epoch =  199, loss = 0.0226)\n",
      "Saving model (epoch =  200, loss = 0.0226)\n",
      "Saving model (epoch =  201, loss = 0.0225)\n",
      "Saving model (epoch =  202, loss = 0.0225)\n",
      "Saving model (epoch =  203, loss = 0.0225)\n",
      "Saving model (epoch =  204, loss = 0.0224)\n",
      "Saving model (epoch =  205, loss = 0.0224)\n",
      "Saving model (epoch =  206, loss = 0.0224)\n",
      "Saving model (epoch =  207, loss = 0.0224)\n",
      "Saving model (epoch =  208, loss = 0.0223)\n",
      "Saving model (epoch =  209, loss = 0.0223)\n",
      "Saving model (epoch =  210, loss = 0.0223)\n",
      "Saving model (epoch =  211, loss = 0.0222)\n",
      "Saving model (epoch =  212, loss = 0.0222)\n",
      "Saving model (epoch =  213, loss = 0.0222)\n",
      "Saving model (epoch =  214, loss = 0.0222)\n",
      "Saving model (epoch =  215, loss = 0.0221)\n",
      "Saving model (epoch =  216, loss = 0.0221)\n",
      "Saving model (epoch =  217, loss = 0.0221)\n",
      "Saving model (epoch =  218, loss = 0.0221)\n",
      "Saving model (epoch =  219, loss = 0.0220)\n",
      "Saving model (epoch =  220, loss = 0.0220)\n",
      "Saving model (epoch =  221, loss = 0.0220)\n",
      "Saving model (epoch =  222, loss = 0.0220)\n",
      "Saving model (epoch =  223, loss = 0.0219)\n",
      "Saving model (epoch =  224, loss = 0.0219)\n",
      "Saving model (epoch =  225, loss = 0.0219)\n",
      "Saving model (epoch =  226, loss = 0.0219)\n",
      "Saving model (epoch =  227, loss = 0.0218)\n",
      "Saving model (epoch =  228, loss = 0.0218)\n",
      "Saving model (epoch =  229, loss = 0.0218)\n",
      "Saving model (epoch =  230, loss = 0.0218)\n",
      "Saving model (epoch =  231, loss = 0.0217)\n",
      "Saving model (epoch =  232, loss = 0.0217)\n",
      "Saving model (epoch =  233, loss = 0.0217)\n",
      "Saving model (epoch =  234, loss = 0.0217)\n",
      "Saving model (epoch =  235, loss = 0.0217)\n",
      "Saving model (epoch =  236, loss = 0.0216)\n",
      "Saving model (epoch =  237, loss = 0.0216)\n",
      "Saving model (epoch =  238, loss = 0.0216)\n",
      "Saving model (epoch =  239, loss = 0.0216)\n",
      "Saving model (epoch =  240, loss = 0.0215)\n",
      "Saving model (epoch =  241, loss = 0.0215)\n",
      "Saving model (epoch =  242, loss = 0.0215)\n",
      "Saving model (epoch =  243, loss = 0.0215)\n",
      "Saving model (epoch =  244, loss = 0.0215)\n",
      "Saving model (epoch =  245, loss = 0.0214)\n",
      "Saving model (epoch =  246, loss = 0.0214)\n",
      "Saving model (epoch =  247, loss = 0.0214)\n",
      "Saving model (epoch =  248, loss = 0.0214)\n",
      "Saving model (epoch =  249, loss = 0.0213)\n",
      "Saving model (epoch =  250, loss = 0.0213)\n",
      "Saving model (epoch =  251, loss = 0.0213)\n",
      "Saving model (epoch =  252, loss = 0.0213)\n",
      "Saving model (epoch =  253, loss = 0.0213)\n",
      "Saving model (epoch =  254, loss = 0.0212)\n",
      "Saving model (epoch =  255, loss = 0.0212)\n",
      "Saving model (epoch =  256, loss = 0.0212)\n",
      "Saving model (epoch =  257, loss = 0.0212)\n",
      "Saving model (epoch =  258, loss = 0.0212)\n",
      "Saving model (epoch =  259, loss = 0.0211)\n",
      "Saving model (epoch =  260, loss = 0.0211)\n",
      "Saving model (epoch =  261, loss = 0.0211)\n",
      "Saving model (epoch =  262, loss = 0.0211)\n",
      "Saving model (epoch =  263, loss = 0.0211)\n",
      "Saving model (epoch =  264, loss = 0.0210)\n",
      "Saving model (epoch =  265, loss = 0.0210)\n",
      "Saving model (epoch =  266, loss = 0.0210)\n",
      "Saving model (epoch =  267, loss = 0.0210)\n",
      "Saving model (epoch =  268, loss = 0.0210)\n",
      "Saving model (epoch =  269, loss = 0.0209)\n",
      "Saving model (epoch =  270, loss = 0.0209)\n",
      "Saving model (epoch =  271, loss = 0.0209)\n",
      "Saving model (epoch =  272, loss = 0.0209)\n",
      "Saving model (epoch =  273, loss = 0.0209)\n",
      "Saving model (epoch =  274, loss = 0.0208)\n",
      "Saving model (epoch =  275, loss = 0.0208)\n",
      "Saving model (epoch =  276, loss = 0.0208)\n",
      "Saving model (epoch =  277, loss = 0.0208)\n",
      "Saving model (epoch =  278, loss = 0.0208)\n",
      "Saving model (epoch =  279, loss = 0.0208)\n",
      "Saving model (epoch =  280, loss = 0.0207)\n",
      "Saving model (epoch =  281, loss = 0.0207)\n",
      "Saving model (epoch =  282, loss = 0.0207)\n",
      "Saving model (epoch =  283, loss = 0.0207)\n",
      "Saving model (epoch =  284, loss = 0.0207)\n",
      "Saving model (epoch =  285, loss = 0.0207)\n",
      "Saving model (epoch =  286, loss = 0.0206)\n",
      "Saving model (epoch =  287, loss = 0.0206)\n",
      "Saving model (epoch =  288, loss = 0.0206)\n",
      "Saving model (epoch =  289, loss = 0.0206)\n",
      "Saving model (epoch =  290, loss = 0.0206)\n",
      "Saving model (epoch =  291, loss = 0.0205)\n",
      "Saving model (epoch =  292, loss = 0.0205)\n",
      "Saving model (epoch =  293, loss = 0.0205)\n",
      "Saving model (epoch =  294, loss = 0.0205)\n",
      "Saving model (epoch =  295, loss = 0.0205)\n",
      "Saving model (epoch =  296, loss = 0.0205)\n",
      "Saving model (epoch =  297, loss = 0.0204)\n",
      "Saving model (epoch =  298, loss = 0.0204)\n",
      "Saving model (epoch =  299, loss = 0.0204)\n",
      "Saving model (epoch =  300, loss = 0.0204)\n",
      "Saving model (epoch =  301, loss = 0.0204)\n",
      "Saving model (epoch =  302, loss = 0.0204)\n",
      "Saving model (epoch =  303, loss = 0.0204)\n",
      "Saving model (epoch =  304, loss = 0.0203)\n",
      "Saving model (epoch =  305, loss = 0.0203)\n",
      "Saving model (epoch =  306, loss = 0.0203)\n",
      "Saving model (epoch =  307, loss = 0.0203)\n",
      "Saving model (epoch =  308, loss = 0.0203)\n",
      "Saving model (epoch =  309, loss = 0.0203)\n",
      "Saving model (epoch =  310, loss = 0.0202)\n",
      "Saving model (epoch =  311, loss = 0.0202)\n",
      "Saving model (epoch =  312, loss = 0.0202)\n",
      "Saving model (epoch =  313, loss = 0.0202)\n",
      "Saving model (epoch =  314, loss = 0.0202)\n",
      "Saving model (epoch =  315, loss = 0.0202)\n",
      "Saving model (epoch =  316, loss = 0.0201)\n",
      "Saving model (epoch =  317, loss = 0.0201)\n",
      "Saving model (epoch =  318, loss = 0.0201)\n",
      "Saving model (epoch =  319, loss = 0.0201)\n",
      "Saving model (epoch =  320, loss = 0.0201)\n",
      "Saving model (epoch =  321, loss = 0.0201)\n",
      "Saving model (epoch =  322, loss = 0.0201)\n",
      "Saving model (epoch =  323, loss = 0.0200)\n",
      "Saving model (epoch =  324, loss = 0.0200)\n",
      "Saving model (epoch =  325, loss = 0.0200)\n",
      "Saving model (epoch =  326, loss = 0.0200)\n",
      "Saving model (epoch =  327, loss = 0.0200)\n",
      "Saving model (epoch =  328, loss = 0.0200)\n",
      "Saving model (epoch =  329, loss = 0.0200)\n",
      "Saving model (epoch =  330, loss = 0.0199)\n",
      "Saving model (epoch =  331, loss = 0.0199)\n",
      "Saving model (epoch =  332, loss = 0.0199)\n",
      "Saving model (epoch =  333, loss = 0.0199)\n",
      "Saving model (epoch =  334, loss = 0.0199)\n",
      "Saving model (epoch =  335, loss = 0.0199)\n",
      "Saving model (epoch =  336, loss = 0.0198)\n",
      "Saving model (epoch =  337, loss = 0.0198)\n",
      "Saving model (epoch =  338, loss = 0.0198)\n",
      "Saving model (epoch =  339, loss = 0.0198)\n",
      "Saving model (epoch =  340, loss = 0.0198)\n",
      "Saving model (epoch =  341, loss = 0.0198)\n",
      "Saving model (epoch =  342, loss = 0.0198)\n",
      "Saving model (epoch =  343, loss = 0.0198)\n",
      "Saving model (epoch =  344, loss = 0.0197)\n",
      "Saving model (epoch =  345, loss = 0.0197)\n",
      "Saving model (epoch =  346, loss = 0.0197)\n",
      "Saving model (epoch =  347, loss = 0.0197)\n",
      "Saving model (epoch =  348, loss = 0.0197)\n",
      "Saving model (epoch =  349, loss = 0.0197)\n",
      "Saving model (epoch =  350, loss = 0.0197)\n",
      "Saving model (epoch =  351, loss = 0.0196)\n",
      "Saving model (epoch =  352, loss = 0.0196)\n",
      "Saving model (epoch =  353, loss = 0.0196)\n",
      "Saving model (epoch =  354, loss = 0.0196)\n",
      "Saving model (epoch =  355, loss = 0.0196)\n",
      "Saving model (epoch =  356, loss = 0.0196)\n",
      "Saving model (epoch =  357, loss = 0.0196)\n",
      "Saving model (epoch =  358, loss = 0.0196)\n",
      "Saving model (epoch =  359, loss = 0.0195)\n",
      "Saving model (epoch =  360, loss = 0.0195)\n",
      "Saving model (epoch =  361, loss = 0.0195)\n",
      "Saving model (epoch =  362, loss = 0.0195)\n",
      "Saving model (epoch =  363, loss = 0.0195)\n",
      "Saving model (epoch =  364, loss = 0.0195)\n",
      "Saving model (epoch =  365, loss = 0.0195)\n",
      "Saving model (epoch =  366, loss = 0.0195)\n",
      "Saving model (epoch =  367, loss = 0.0194)\n",
      "Saving model (epoch =  368, loss = 0.0194)\n",
      "Saving model (epoch =  369, loss = 0.0194)\n",
      "Saving model (epoch =  370, loss = 0.0194)\n",
      "Saving model (epoch =  371, loss = 0.0194)\n",
      "Saving model (epoch =  372, loss = 0.0194)\n",
      "Saving model (epoch =  373, loss = 0.0194)\n",
      "Saving model (epoch =  374, loss = 0.0193)\n",
      "Saving model (epoch =  375, loss = 0.0193)\n",
      "Saving model (epoch =  376, loss = 0.0193)\n",
      "Saving model (epoch =  377, loss = 0.0193)\n",
      "Saving model (epoch =  378, loss = 0.0193)\n",
      "Saving model (epoch =  379, loss = 0.0193)\n",
      "Saving model (epoch =  380, loss = 0.0193)\n",
      "Saving model (epoch =  381, loss = 0.0193)\n",
      "Saving model (epoch =  382, loss = 0.0193)\n",
      "Saving model (epoch =  383, loss = 0.0192)\n",
      "Saving model (epoch =  384, loss = 0.0192)\n",
      "Saving model (epoch =  385, loss = 0.0192)\n",
      "Saving model (epoch =  386, loss = 0.0192)\n",
      "Saving model (epoch =  387, loss = 0.0192)\n",
      "Saving model (epoch =  388, loss = 0.0192)\n",
      "Saving model (epoch =  389, loss = 0.0192)\n",
      "Saving model (epoch =  390, loss = 0.0192)\n",
      "Saving model (epoch =  391, loss = 0.0191)\n",
      "Saving model (epoch =  392, loss = 0.0191)\n",
      "Saving model (epoch =  393, loss = 0.0191)\n",
      "Saving model (epoch =  394, loss = 0.0191)\n",
      "Saving model (epoch =  395, loss = 0.0191)\n",
      "Saving model (epoch =  396, loss = 0.0191)\n",
      "Saving model (epoch =  397, loss = 0.0191)\n",
      "Saving model (epoch =  398, loss = 0.0191)\n",
      "Saving model (epoch =  399, loss = 0.0191)\n",
      "Saving model (epoch =  400, loss = 0.0191)\n",
      "Saving model (epoch =  401, loss = 0.0190)\n",
      "Saving model (epoch =  402, loss = 0.0190)\n",
      "Saving model (epoch =  403, loss = 0.0190)\n",
      "Saving model (epoch =  404, loss = 0.0190)\n",
      "Saving model (epoch =  405, loss = 0.0190)\n",
      "Saving model (epoch =  406, loss = 0.0190)\n",
      "Saving model (epoch =  407, loss = 0.0190)\n",
      "Saving model (epoch =  408, loss = 0.0190)\n",
      "Saving model (epoch =  409, loss = 0.0189)\n",
      "Saving model (epoch =  410, loss = 0.0189)\n",
      "Saving model (epoch =  411, loss = 0.0189)\n",
      "Saving model (epoch =  412, loss = 0.0189)\n",
      "Saving model (epoch =  413, loss = 0.0189)\n",
      "Saving model (epoch =  414, loss = 0.0189)\n",
      "Saving model (epoch =  415, loss = 0.0189)\n",
      "Saving model (epoch =  416, loss = 0.0189)\n",
      "Saving model (epoch =  417, loss = 0.0189)\n",
      "Saving model (epoch =  418, loss = 0.0188)\n",
      "Saving model (epoch =  419, loss = 0.0188)\n",
      "Saving model (epoch =  420, loss = 0.0188)\n",
      "Saving model (epoch =  421, loss = 0.0188)\n",
      "Saving model (epoch =  422, loss = 0.0188)\n",
      "Saving model (epoch =  423, loss = 0.0188)\n",
      "Saving model (epoch =  424, loss = 0.0188)\n",
      "Saving model (epoch =  425, loss = 0.0188)\n",
      "Saving model (epoch =  426, loss = 0.0188)\n",
      "Saving model (epoch =  427, loss = 0.0188)\n",
      "Saving model (epoch =  428, loss = 0.0187)\n",
      "Saving model (epoch =  429, loss = 0.0187)\n",
      "Saving model (epoch =  430, loss = 0.0187)\n",
      "Saving model (epoch =  431, loss = 0.0187)\n",
      "Saving model (epoch =  432, loss = 0.0187)\n",
      "Saving model (epoch =  433, loss = 0.0187)\n",
      "Saving model (epoch =  434, loss = 0.0187)\n",
      "Saving model (epoch =  435, loss = 0.0187)\n",
      "Saving model (epoch =  436, loss = 0.0187)\n",
      "Saving model (epoch =  437, loss = 0.0187)\n",
      "Saving model (epoch =  438, loss = 0.0186)\n",
      "Saving model (epoch =  439, loss = 0.0186)\n",
      "Saving model (epoch =  440, loss = 0.0186)\n",
      "Saving model (epoch =  441, loss = 0.0186)\n",
      "Saving model (epoch =  442, loss = 0.0186)\n",
      "Saving model (epoch =  443, loss = 0.0186)\n",
      "Saving model (epoch =  444, loss = 0.0186)\n",
      "Saving model (epoch =  445, loss = 0.0186)\n",
      "Saving model (epoch =  446, loss = 0.0186)\n",
      "Saving model (epoch =  447, loss = 0.0186)\n",
      "Saving model (epoch =  448, loss = 0.0185)\n",
      "Saving model (epoch =  449, loss = 0.0185)\n",
      "Saving model (epoch =  450, loss = 0.0185)\n",
      "Saving model (epoch =  451, loss = 0.0185)\n",
      "Saving model (epoch =  452, loss = 0.0185)\n",
      "Saving model (epoch =  453, loss = 0.0185)\n",
      "Saving model (epoch =  454, loss = 0.0185)\n",
      "Saving model (epoch =  455, loss = 0.0185)\n",
      "Saving model (epoch =  456, loss = 0.0185)\n",
      "Saving model (epoch =  457, loss = 0.0185)\n",
      "Saving model (epoch =  458, loss = 0.0185)\n",
      "Saving model (epoch =  459, loss = 0.0184)\n",
      "Saving model (epoch =  460, loss = 0.0184)\n",
      "Saving model (epoch =  461, loss = 0.0184)\n",
      "Saving model (epoch =  462, loss = 0.0184)\n",
      "Saving model (epoch =  463, loss = 0.0184)\n",
      "Saving model (epoch =  464, loss = 0.0184)\n",
      "Saving model (epoch =  465, loss = 0.0184)\n",
      "Saving model (epoch =  466, loss = 0.0184)\n",
      "Saving model (epoch =  467, loss = 0.0184)\n",
      "Saving model (epoch =  468, loss = 0.0184)\n",
      "Saving model (epoch =  469, loss = 0.0183)\n",
      "Saving model (epoch =  470, loss = 0.0183)\n",
      "Saving model (epoch =  471, loss = 0.0183)\n",
      "Saving model (epoch =  472, loss = 0.0183)\n",
      "Saving model (epoch =  473, loss = 0.0183)\n",
      "Saving model (epoch =  474, loss = 0.0183)\n",
      "Saving model (epoch =  475, loss = 0.0183)\n",
      "Saving model (epoch =  476, loss = 0.0183)\n",
      "Saving model (epoch =  477, loss = 0.0183)\n",
      "Saving model (epoch =  478, loss = 0.0183)\n",
      "Saving model (epoch =  479, loss = 0.0183)\n",
      "Saving model (epoch =  480, loss = 0.0182)\n",
      "Saving model (epoch =  481, loss = 0.0182)\n",
      "Saving model (epoch =  482, loss = 0.0182)\n",
      "Saving model (epoch =  483, loss = 0.0182)\n",
      "Saving model (epoch =  485, loss = 0.0182)\n",
      "Saving model (epoch =  486, loss = 0.0182)\n",
      "Saving model (epoch =  487, loss = 0.0182)\n",
      "Saving model (epoch =  488, loss = 0.0182)\n",
      "Saving model (epoch =  489, loss = 0.0182)\n",
      "Saving model (epoch =  490, loss = 0.0182)\n",
      "Saving model (epoch =  491, loss = 0.0182)\n",
      "Saving model (epoch =  492, loss = 0.0181)\n",
      "Saving model (epoch =  493, loss = 0.0181)\n",
      "Saving model (epoch =  494, loss = 0.0181)\n",
      "Saving model (epoch =  495, loss = 0.0181)\n",
      "Saving model (epoch =  496, loss = 0.0181)\n",
      "Saving model (epoch =  497, loss = 0.0181)\n",
      "Saving model (epoch =  498, loss = 0.0181)\n",
      "Saving model (epoch =  499, loss = 0.0181)\n",
      "Saving model (epoch =  500, loss = 0.0181)\n",
      "Saving model (epoch =  501, loss = 0.0181)\n",
      "Saving model (epoch =  502, loss = 0.0181)\n",
      "Saving model (epoch =  503, loss = 0.0181)\n",
      "Saving model (epoch =  504, loss = 0.0181)\n",
      "Saving model (epoch =  505, loss = 0.0180)\n",
      "Saving model (epoch =  506, loss = 0.0180)\n",
      "Saving model (epoch =  507, loss = 0.0180)\n",
      "Saving model (epoch =  508, loss = 0.0180)\n",
      "Saving model (epoch =  509, loss = 0.0180)\n",
      "Saving model (epoch =  510, loss = 0.0180)\n",
      "Saving model (epoch =  511, loss = 0.0180)\n",
      "Saving model (epoch =  512, loss = 0.0180)\n",
      "Saving model (epoch =  513, loss = 0.0180)\n",
      "Saving model (epoch =  514, loss = 0.0180)\n",
      "Saving model (epoch =  515, loss = 0.0180)\n",
      "Saving model (epoch =  516, loss = 0.0179)\n",
      "Saving model (epoch =  517, loss = 0.0179)\n",
      "Saving model (epoch =  518, loss = 0.0179)\n",
      "Saving model (epoch =  519, loss = 0.0179)\n",
      "Saving model (epoch =  520, loss = 0.0179)\n",
      "Saving model (epoch =  521, loss = 0.0179)\n",
      "Saving model (epoch =  523, loss = 0.0179)\n",
      "Saving model (epoch =  524, loss = 0.0179)\n",
      "Saving model (epoch =  525, loss = 0.0179)\n",
      "Saving model (epoch =  526, loss = 0.0179)\n",
      "Saving model (epoch =  527, loss = 0.0179)\n",
      "Saving model (epoch =  529, loss = 0.0178)\n",
      "Saving model (epoch =  531, loss = 0.0178)\n",
      "Saving model (epoch =  532, loss = 0.0178)\n",
      "Saving model (epoch =  533, loss = 0.0178)\n",
      "Saving model (epoch =  534, loss = 0.0178)\n",
      "Saving model (epoch =  536, loss = 0.0178)\n",
      "Saving model (epoch =  537, loss = 0.0178)\n",
      "Saving model (epoch =  538, loss = 0.0178)\n",
      "Saving model (epoch =  539, loss = 0.0178)\n",
      "Saving model (epoch =  540, loss = 0.0178)\n",
      "Saving model (epoch =  541, loss = 0.0178)\n",
      "Saving model (epoch =  542, loss = 0.0177)\n",
      "Saving model (epoch =  543, loss = 0.0177)\n",
      "Saving model (epoch =  544, loss = 0.0177)\n",
      "Saving model (epoch =  545, loss = 0.0177)\n",
      "Saving model (epoch =  546, loss = 0.0177)\n",
      "Saving model (epoch =  547, loss = 0.0177)\n",
      "Saving model (epoch =  548, loss = 0.0177)\n",
      "Saving model (epoch =  549, loss = 0.0177)\n",
      "Saving model (epoch =  551, loss = 0.0177)\n",
      "Saving model (epoch =  552, loss = 0.0177)\n",
      "Saving model (epoch =  554, loss = 0.0177)\n",
      "Saving model (epoch =  555, loss = 0.0176)\n",
      "Saving model (epoch =  556, loss = 0.0176)\n",
      "Saving model (epoch =  557, loss = 0.0176)\n",
      "Saving model (epoch =  558, loss = 0.0176)\n",
      "Saving model (epoch =  559, loss = 0.0176)\n",
      "Saving model (epoch =  560, loss = 0.0176)\n",
      "Saving model (epoch =  561, loss = 0.0176)\n",
      "Saving model (epoch =  563, loss = 0.0176)\n",
      "Saving model (epoch =  564, loss = 0.0176)\n",
      "Saving model (epoch =  565, loss = 0.0176)\n",
      "Saving model (epoch =  566, loss = 0.0176)\n",
      "Saving model (epoch =  567, loss = 0.0176)\n",
      "Saving model (epoch =  568, loss = 0.0176)\n",
      "Saving model (epoch =  569, loss = 0.0176)\n",
      "Saving model (epoch =  570, loss = 0.0175)\n",
      "Saving model (epoch =  571, loss = 0.0175)\n",
      "Saving model (epoch =  572, loss = 0.0175)\n",
      "Saving model (epoch =  573, loss = 0.0175)\n",
      "Saving model (epoch =  574, loss = 0.0175)\n",
      "Saving model (epoch =  575, loss = 0.0175)\n",
      "Saving model (epoch =  576, loss = 0.0175)\n",
      "Saving model (epoch =  577, loss = 0.0175)\n",
      "Saving model (epoch =  578, loss = 0.0175)\n",
      "Saving model (epoch =  579, loss = 0.0175)\n",
      "Saving model (epoch =  580, loss = 0.0175)\n",
      "Saving model (epoch =  581, loss = 0.0175)\n",
      "Saving model (epoch =  582, loss = 0.0175)\n",
      "Saving model (epoch =  583, loss = 0.0175)\n",
      "Saving model (epoch =  584, loss = 0.0174)\n",
      "Saving model (epoch =  585, loss = 0.0174)\n",
      "Saving model (epoch =  586, loss = 0.0174)\n",
      "Saving model (epoch =  587, loss = 0.0174)\n",
      "Saving model (epoch =  588, loss = 0.0174)\n",
      "Saving model (epoch =  589, loss = 0.0174)\n",
      "Saving model (epoch =  590, loss = 0.0174)\n",
      "Saving model (epoch =  591, loss = 0.0174)\n",
      "Saving model (epoch =  592, loss = 0.0174)\n",
      "Saving model (epoch =  593, loss = 0.0174)\n",
      "Saving model (epoch =  594, loss = 0.0174)\n",
      "Saving model (epoch =  595, loss = 0.0174)\n",
      "Saving model (epoch =  596, loss = 0.0174)\n",
      "Saving model (epoch =  597, loss = 0.0174)\n",
      "Saving model (epoch =  598, loss = 0.0174)\n",
      "Saving model (epoch =  599, loss = 0.0173)\n",
      "Saving model (epoch =  600, loss = 0.0173)\n",
      "Saving model (epoch =  602, loss = 0.0173)\n",
      "Saving model (epoch =  603, loss = 0.0173)\n",
      "Saving model (epoch =  604, loss = 0.0173)\n",
      "Saving model (epoch =  605, loss = 0.0173)\n",
      "Saving model (epoch =  606, loss = 0.0173)\n",
      "Saving model (epoch =  607, loss = 0.0173)\n",
      "Saving model (epoch =  609, loss = 0.0173)\n",
      "Saving model (epoch =  610, loss = 0.0173)\n",
      "Saving model (epoch =  611, loss = 0.0173)\n",
      "Saving model (epoch =  612, loss = 0.0173)\n",
      "Saving model (epoch =  613, loss = 0.0173)\n",
      "Saving model (epoch =  614, loss = 0.0172)\n",
      "Saving model (epoch =  616, loss = 0.0172)\n",
      "Saving model (epoch =  617, loss = 0.0172)\n",
      "Saving model (epoch =  618, loss = 0.0172)\n",
      "Saving model (epoch =  619, loss = 0.0172)\n",
      "Saving model (epoch =  620, loss = 0.0172)\n",
      "Saving model (epoch =  621, loss = 0.0172)\n",
      "Saving model (epoch =  622, loss = 0.0172)\n",
      "Saving model (epoch =  623, loss = 0.0172)\n",
      "Saving model (epoch =  624, loss = 0.0172)\n",
      "Saving model (epoch =  625, loss = 0.0172)\n",
      "Saving model (epoch =  626, loss = 0.0172)\n",
      "Saving model (epoch =  627, loss = 0.0172)\n",
      "Saving model (epoch =  628, loss = 0.0172)\n",
      "Saving model (epoch =  629, loss = 0.0172)\n",
      "Saving model (epoch =  630, loss = 0.0172)\n",
      "Saving model (epoch =  631, loss = 0.0171)\n",
      "Saving model (epoch =  633, loss = 0.0171)\n",
      "Saving model (epoch =  634, loss = 0.0171)\n",
      "Saving model (epoch =  636, loss = 0.0171)\n",
      "Saving model (epoch =  637, loss = 0.0171)\n",
      "Saving model (epoch =  638, loss = 0.0171)\n",
      "Saving model (epoch =  639, loss = 0.0171)\n",
      "Saving model (epoch =  640, loss = 0.0171)\n",
      "Saving model (epoch =  641, loss = 0.0171)\n",
      "Saving model (epoch =  642, loss = 0.0171)\n",
      "Saving model (epoch =  643, loss = 0.0171)\n",
      "Saving model (epoch =  644, loss = 0.0171)\n",
      "Saving model (epoch =  645, loss = 0.0171)\n",
      "Saving model (epoch =  646, loss = 0.0171)\n",
      "Saving model (epoch =  647, loss = 0.0170)\n",
      "Saving model (epoch =  648, loss = 0.0170)\n",
      "Saving model (epoch =  649, loss = 0.0170)\n",
      "Saving model (epoch =  650, loss = 0.0170)\n",
      "Saving model (epoch =  651, loss = 0.0170)\n",
      "Saving model (epoch =  652, loss = 0.0170)\n",
      "Saving model (epoch =  653, loss = 0.0170)\n",
      "Saving model (epoch =  654, loss = 0.0170)\n",
      "Saving model (epoch =  655, loss = 0.0170)\n",
      "Saving model (epoch =  656, loss = 0.0170)\n",
      "Saving model (epoch =  657, loss = 0.0170)\n",
      "Saving model (epoch =  658, loss = 0.0170)\n",
      "Saving model (epoch =  659, loss = 0.0170)\n",
      "Saving model (epoch =  660, loss = 0.0170)\n",
      "Saving model (epoch =  661, loss = 0.0170)\n",
      "Saving model (epoch =  662, loss = 0.0170)\n",
      "Saving model (epoch =  663, loss = 0.0170)\n",
      "Saving model (epoch =  664, loss = 0.0169)\n",
      "Saving model (epoch =  665, loss = 0.0169)\n",
      "Saving model (epoch =  666, loss = 0.0169)\n",
      "Saving model (epoch =  667, loss = 0.0169)\n",
      "Saving model (epoch =  668, loss = 0.0169)\n",
      "Saving model (epoch =  669, loss = 0.0169)\n",
      "Saving model (epoch =  671, loss = 0.0169)\n",
      "Saving model (epoch =  672, loss = 0.0169)\n",
      "Saving model (epoch =  673, loss = 0.0169)\n",
      "Saving model (epoch =  674, loss = 0.0169)\n",
      "Saving model (epoch =  675, loss = 0.0169)\n",
      "Saving model (epoch =  676, loss = 0.0169)\n",
      "Saving model (epoch =  677, loss = 0.0169)\n",
      "Saving model (epoch =  678, loss = 0.0169)\n",
      "Saving model (epoch =  679, loss = 0.0169)\n",
      "Saving model (epoch =  680, loss = 0.0169)\n",
      "Saving model (epoch =  681, loss = 0.0168)\n",
      "Saving model (epoch =  682, loss = 0.0168)\n",
      "Saving model (epoch =  684, loss = 0.0168)\n",
      "Saving model (epoch =  685, loss = 0.0168)\n",
      "Saving model (epoch =  686, loss = 0.0168)\n",
      "Saving model (epoch =  687, loss = 0.0168)\n",
      "Saving model (epoch =  688, loss = 0.0168)\n",
      "Saving model (epoch =  689, loss = 0.0168)\n",
      "Saving model (epoch =  690, loss = 0.0168)\n",
      "Saving model (epoch =  691, loss = 0.0168)\n",
      "Saving model (epoch =  692, loss = 0.0168)\n",
      "Saving model (epoch =  694, loss = 0.0168)\n",
      "Saving model (epoch =  695, loss = 0.0168)\n",
      "Saving model (epoch =  696, loss = 0.0168)\n",
      "Saving model (epoch =  697, loss = 0.0168)\n",
      "Saving model (epoch =  698, loss = 0.0168)\n",
      "Saving model (epoch =  699, loss = 0.0167)\n",
      "Saving model (epoch =  700, loss = 0.0167)\n",
      "Saving model (epoch =  701, loss = 0.0167)\n",
      "Saving model (epoch =  702, loss = 0.0167)\n",
      "Saving model (epoch =  704, loss = 0.0167)\n",
      "Saving model (epoch =  705, loss = 0.0167)\n",
      "Saving model (epoch =  706, loss = 0.0167)\n",
      "Saving model (epoch =  707, loss = 0.0167)\n",
      "Saving model (epoch =  708, loss = 0.0167)\n",
      "Saving model (epoch =  709, loss = 0.0167)\n",
      "Saving model (epoch =  710, loss = 0.0167)\n",
      "Saving model (epoch =  711, loss = 0.0167)\n",
      "Saving model (epoch =  712, loss = 0.0167)\n",
      "Saving model (epoch =  713, loss = 0.0167)\n",
      "Saving model (epoch =  714, loss = 0.0167)\n",
      "Saving model (epoch =  715, loss = 0.0167)\n",
      "Saving model (epoch =  716, loss = 0.0167)\n",
      "Saving model (epoch =  717, loss = 0.0167)\n",
      "Saving model (epoch =  718, loss = 0.0167)\n",
      "Saving model (epoch =  719, loss = 0.0166)\n",
      "Saving model (epoch =  720, loss = 0.0166)\n",
      "Saving model (epoch =  721, loss = 0.0166)\n",
      "Saving model (epoch =  722, loss = 0.0166)\n",
      "Saving model (epoch =  723, loss = 0.0166)\n",
      "Saving model (epoch =  724, loss = 0.0166)\n",
      "Saving model (epoch =  725, loss = 0.0166)\n",
      "Saving model (epoch =  726, loss = 0.0166)\n",
      "Saving model (epoch =  727, loss = 0.0166)\n",
      "Saving model (epoch =  729, loss = 0.0166)\n",
      "Saving model (epoch =  730, loss = 0.0166)\n",
      "Saving model (epoch =  731, loss = 0.0166)\n",
      "Saving model (epoch =  732, loss = 0.0166)\n",
      "Saving model (epoch =  733, loss = 0.0166)\n",
      "Saving model (epoch =  734, loss = 0.0166)\n",
      "Saving model (epoch =  735, loss = 0.0166)\n",
      "Saving model (epoch =  736, loss = 0.0166)\n",
      "Saving model (epoch =  737, loss = 0.0165)\n",
      "Saving model (epoch =  739, loss = 0.0165)\n",
      "Saving model (epoch =  740, loss = 0.0165)\n",
      "Saving model (epoch =  741, loss = 0.0165)\n",
      "Saving model (epoch =  743, loss = 0.0165)\n",
      "Saving model (epoch =  744, loss = 0.0165)\n",
      "Saving model (epoch =  745, loss = 0.0165)\n",
      "Saving model (epoch =  746, loss = 0.0165)\n",
      "Saving model (epoch =  747, loss = 0.0165)\n",
      "Saving model (epoch =  748, loss = 0.0165)\n",
      "Saving model (epoch =  749, loss = 0.0165)\n",
      "Saving model (epoch =  750, loss = 0.0165)\n",
      "Saving model (epoch =  751, loss = 0.0165)\n",
      "Saving model (epoch =  752, loss = 0.0165)\n",
      "Saving model (epoch =  753, loss = 0.0165)\n",
      "Saving model (epoch =  754, loss = 0.0165)\n",
      "Saving model (epoch =  755, loss = 0.0165)\n",
      "Saving model (epoch =  756, loss = 0.0165)\n",
      "Saving model (epoch =  757, loss = 0.0165)\n",
      "Saving model (epoch =  758, loss = 0.0164)\n",
      "Saving model (epoch =  759, loss = 0.0164)\n",
      "Saving model (epoch =  760, loss = 0.0164)\n",
      "Saving model (epoch =  761, loss = 0.0164)\n",
      "Saving model (epoch =  763, loss = 0.0164)\n",
      "Saving model (epoch =  764, loss = 0.0164)\n",
      "Saving model (epoch =  765, loss = 0.0164)\n",
      "Saving model (epoch =  767, loss = 0.0164)\n",
      "Saving model (epoch =  768, loss = 0.0164)\n",
      "Saving model (epoch =  769, loss = 0.0164)\n",
      "Saving model (epoch =  771, loss = 0.0164)\n",
      "Saving model (epoch =  772, loss = 0.0164)\n",
      "Saving model (epoch =  773, loss = 0.0164)\n",
      "Saving model (epoch =  774, loss = 0.0164)\n",
      "Saving model (epoch =  775, loss = 0.0164)\n",
      "Saving model (epoch =  777, loss = 0.0164)\n",
      "Saving model (epoch =  778, loss = 0.0164)\n",
      "Saving model (epoch =  779, loss = 0.0163)\n",
      "Saving model (epoch =  781, loss = 0.0163)\n",
      "Saving model (epoch =  782, loss = 0.0163)\n",
      "Saving model (epoch =  784, loss = 0.0163)\n",
      "Saving model (epoch =  785, loss = 0.0163)\n",
      "Saving model (epoch =  786, loss = 0.0163)\n",
      "Saving model (epoch =  788, loss = 0.0163)\n",
      "Saving model (epoch =  789, loss = 0.0163)\n",
      "Saving model (epoch =  792, loss = 0.0163)\n",
      "Saving model (epoch =  793, loss = 0.0163)\n",
      "Saving model (epoch =  794, loss = 0.0163)\n",
      "Saving model (epoch =  795, loss = 0.0163)\n",
      "Saving model (epoch =  796, loss = 0.0163)\n",
      "Saving model (epoch =  798, loss = 0.0163)\n",
      "Saving model (epoch =  799, loss = 0.0163)\n",
      "Saving model (epoch =  800, loss = 0.0162)\n",
      "Saving model (epoch =  801, loss = 0.0162)\n",
      "Saving model (epoch =  802, loss = 0.0162)\n",
      "Saving model (epoch =  803, loss = 0.0162)\n",
      "Saving model (epoch =  805, loss = 0.0162)\n",
      "Saving model (epoch =  806, loss = 0.0162)\n",
      "Saving model (epoch =  807, loss = 0.0162)\n",
      "Saving model (epoch =  808, loss = 0.0162)\n",
      "Saving model (epoch =  809, loss = 0.0162)\n",
      "Saving model (epoch =  810, loss = 0.0162)\n",
      "Saving model (epoch =  811, loss = 0.0162)\n",
      "Saving model (epoch =  812, loss = 0.0162)\n",
      "Saving model (epoch =  813, loss = 0.0162)\n",
      "Saving model (epoch =  814, loss = 0.0162)\n",
      "Saving model (epoch =  816, loss = 0.0162)\n",
      "Saving model (epoch =  818, loss = 0.0162)\n",
      "Saving model (epoch =  819, loss = 0.0162)\n",
      "Saving model (epoch =  820, loss = 0.0162)\n",
      "Saving model (epoch =  821, loss = 0.0162)\n",
      "Saving model (epoch =  822, loss = 0.0162)\n",
      "Saving model (epoch =  823, loss = 0.0161)\n",
      "Saving model (epoch =  824, loss = 0.0161)\n",
      "Saving model (epoch =  825, loss = 0.0161)\n",
      "Saving model (epoch =  826, loss = 0.0161)\n",
      "Saving model (epoch =  827, loss = 0.0161)\n",
      "Saving model (epoch =  828, loss = 0.0161)\n",
      "Saving model (epoch =  829, loss = 0.0161)\n",
      "Saving model (epoch =  830, loss = 0.0161)\n",
      "Saving model (epoch =  831, loss = 0.0161)\n",
      "Saving model (epoch =  832, loss = 0.0161)\n",
      "Saving model (epoch =  833, loss = 0.0161)\n",
      "Saving model (epoch =  834, loss = 0.0161)\n",
      "Saving model (epoch =  835, loss = 0.0161)\n",
      "Saving model (epoch =  836, loss = 0.0161)\n",
      "Saving model (epoch =  837, loss = 0.0161)\n",
      "Saving model (epoch =  838, loss = 0.0161)\n",
      "Saving model (epoch =  839, loss = 0.0161)\n",
      "Saving model (epoch =  840, loss = 0.0161)\n",
      "Saving model (epoch =  841, loss = 0.0161)\n",
      "Saving model (epoch =  843, loss = 0.0161)\n",
      "Saving model (epoch =  846, loss = 0.0161)\n",
      "Saving model (epoch =  847, loss = 0.0160)\n",
      "Saving model (epoch =  848, loss = 0.0160)\n",
      "Saving model (epoch =  849, loss = 0.0160)\n",
      "Saving model (epoch =  850, loss = 0.0160)\n",
      "Saving model (epoch =  851, loss = 0.0160)\n",
      "Saving model (epoch =  852, loss = 0.0160)\n",
      "Saving model (epoch =  854, loss = 0.0160)\n",
      "Saving model (epoch =  855, loss = 0.0160)\n",
      "Saving model (epoch =  856, loss = 0.0160)\n",
      "Saving model (epoch =  857, loss = 0.0160)\n",
      "Saving model (epoch =  859, loss = 0.0160)\n",
      "Saving model (epoch =  860, loss = 0.0160)\n",
      "Saving model (epoch =  861, loss = 0.0160)\n",
      "Saving model (epoch =  862, loss = 0.0160)\n",
      "Saving model (epoch =  863, loss = 0.0160)\n",
      "Saving model (epoch =  864, loss = 0.0160)\n",
      "Saving model (epoch =  865, loss = 0.0160)\n",
      "Saving model (epoch =  866, loss = 0.0160)\n",
      "Saving model (epoch =  868, loss = 0.0160)\n",
      "Saving model (epoch =  869, loss = 0.0160)\n",
      "Saving model (epoch =  870, loss = 0.0160)\n",
      "Saving model (epoch =  871, loss = 0.0160)\n",
      "Saving model (epoch =  872, loss = 0.0159)\n",
      "Saving model (epoch =  873, loss = 0.0159)\n",
      "Saving model (epoch =  874, loss = 0.0159)\n",
      "Saving model (epoch =  877, loss = 0.0159)\n",
      "Saving model (epoch =  878, loss = 0.0159)\n",
      "Saving model (epoch =  879, loss = 0.0159)\n",
      "Saving model (epoch =  880, loss = 0.0159)\n",
      "Saving model (epoch =  881, loss = 0.0159)\n",
      "Saving model (epoch =  882, loss = 0.0159)\n",
      "Saving model (epoch =  883, loss = 0.0159)\n",
      "Saving model (epoch =  884, loss = 0.0159)\n",
      "Saving model (epoch =  885, loss = 0.0159)\n",
      "Saving model (epoch =  888, loss = 0.0159)\n",
      "Saving model (epoch =  889, loss = 0.0159)\n",
      "Saving model (epoch =  890, loss = 0.0159)\n",
      "Saving model (epoch =  891, loss = 0.0159)\n",
      "Saving model (epoch =  892, loss = 0.0159)\n",
      "Saving model (epoch =  893, loss = 0.0159)\n",
      "Saving model (epoch =  894, loss = 0.0159)\n",
      "Saving model (epoch =  896, loss = 0.0159)\n",
      "Saving model (epoch =  898, loss = 0.0158)\n",
      "Saving model (epoch =  899, loss = 0.0158)\n",
      "Saving model (epoch =  900, loss = 0.0158)\n",
      "Saving model (epoch =  901, loss = 0.0158)\n",
      "Saving model (epoch =  902, loss = 0.0158)\n",
      "Saving model (epoch =  903, loss = 0.0158)\n",
      "Saving model (epoch =  905, loss = 0.0158)\n",
      "Saving model (epoch =  906, loss = 0.0158)\n",
      "Saving model (epoch =  907, loss = 0.0158)\n",
      "Saving model (epoch =  910, loss = 0.0158)\n",
      "Saving model (epoch =  911, loss = 0.0158)\n",
      "Saving model (epoch =  912, loss = 0.0158)\n",
      "Saving model (epoch =  914, loss = 0.0158)\n",
      "Saving model (epoch =  915, loss = 0.0158)\n",
      "Saving model (epoch =  916, loss = 0.0158)\n",
      "Saving model (epoch =  918, loss = 0.0158)\n",
      "Saving model (epoch =  919, loss = 0.0158)\n",
      "Saving model (epoch =  921, loss = 0.0158)\n",
      "Saving model (epoch =  922, loss = 0.0158)\n",
      "Saving model (epoch =  924, loss = 0.0157)\n",
      "Saving model (epoch =  925, loss = 0.0157)\n",
      "Saving model (epoch =  927, loss = 0.0157)\n",
      "Saving model (epoch =  929, loss = 0.0157)\n",
      "Saving model (epoch =  930, loss = 0.0157)\n",
      "Saving model (epoch =  931, loss = 0.0157)\n",
      "Saving model (epoch =  933, loss = 0.0157)\n",
      "Saving model (epoch =  934, loss = 0.0157)\n",
      "Saving model (epoch =  935, loss = 0.0157)\n",
      "Saving model (epoch =  937, loss = 0.0157)\n",
      "Saving model (epoch =  938, loss = 0.0157)\n",
      "Saving model (epoch =  940, loss = 0.0157)\n",
      "Saving model (epoch =  941, loss = 0.0157)\n",
      "Saving model (epoch =  942, loss = 0.0157)\n",
      "Saving model (epoch =  943, loss = 0.0157)\n",
      "Saving model (epoch =  944, loss = 0.0157)\n",
      "Saving model (epoch =  945, loss = 0.0157)\n",
      "Saving model (epoch =  946, loss = 0.0157)\n",
      "Saving model (epoch =  947, loss = 0.0157)\n",
      "Saving model (epoch =  948, loss = 0.0157)\n",
      "Saving model (epoch =  949, loss = 0.0157)\n",
      "Saving model (epoch =  950, loss = 0.0157)\n",
      "Saving model (epoch =  951, loss = 0.0157)\n",
      "Saving model (epoch =  952, loss = 0.0157)\n",
      "Saving model (epoch =  953, loss = 0.0156)\n",
      "Saving model (epoch =  954, loss = 0.0156)\n",
      "Saving model (epoch =  955, loss = 0.0156)\n",
      "Saving model (epoch =  956, loss = 0.0156)\n",
      "Saving model (epoch =  957, loss = 0.0156)\n",
      "Saving model (epoch =  959, loss = 0.0156)\n",
      "Saving model (epoch =  960, loss = 0.0156)\n",
      "Saving model (epoch =  961, loss = 0.0156)\n",
      "Saving model (epoch =  963, loss = 0.0156)\n",
      "Saving model (epoch =  964, loss = 0.0156)\n",
      "Saving model (epoch =  966, loss = 0.0156)\n",
      "Saving model (epoch =  968, loss = 0.0156)\n",
      "Saving model (epoch =  969, loss = 0.0156)\n",
      "Saving model (epoch =  970, loss = 0.0156)\n",
      "Saving model (epoch =  971, loss = 0.0156)\n",
      "Saving model (epoch =  972, loss = 0.0156)\n",
      "Saving model (epoch =  973, loss = 0.0156)\n",
      "Saving model (epoch =  974, loss = 0.0156)\n",
      "Saving model (epoch =  975, loss = 0.0156)\n",
      "Saving model (epoch =  976, loss = 0.0156)\n",
      "Saving model (epoch =  978, loss = 0.0156)\n",
      "Saving model (epoch =  980, loss = 0.0155)\n",
      "Saving model (epoch =  984, loss = 0.0155)\n",
      "Saving model (epoch =  985, loss = 0.0155)\n",
      "Saving model (epoch =  986, loss = 0.0155)\n",
      "Saving model (epoch =  988, loss = 0.0155)\n",
      "Saving model (epoch =  989, loss = 0.0155)\n",
      "Saving model (epoch =  991, loss = 0.0155)\n",
      "Saving model (epoch =  992, loss = 0.0155)\n",
      "Saving model (epoch =  995, loss = 0.0155)\n",
      "Saving model (epoch =  996, loss = 0.0155)\n",
      "Saving model (epoch =  997, loss = 0.0155)\n",
      "Saving model (epoch =  999, loss = 0.0155)\n",
      "Saving model (epoch = 1001, loss = 0.0155)\n",
      "Saving model (epoch = 1002, loss = 0.0155)\n",
      "Saving model (epoch = 1003, loss = 0.0155)\n",
      "Saving model (epoch = 1004, loss = 0.0155)\n",
      "Saving model (epoch = 1005, loss = 0.0155)\n",
      "Saving model (epoch = 1007, loss = 0.0155)\n",
      "Saving model (epoch = 1009, loss = 0.0155)\n",
      "Saving model (epoch = 1010, loss = 0.0155)\n",
      "Saving model (epoch = 1011, loss = 0.0155)\n",
      "Saving model (epoch = 1012, loss = 0.0155)\n",
      "Saving model (epoch = 1013, loss = 0.0155)\n",
      "Saving model (epoch = 1014, loss = 0.0154)\n",
      "Saving model (epoch = 1015, loss = 0.0154)\n",
      "Saving model (epoch = 1016, loss = 0.0154)\n",
      "Saving model (epoch = 1018, loss = 0.0154)\n",
      "Saving model (epoch = 1019, loss = 0.0154)\n",
      "Saving model (epoch = 1021, loss = 0.0154)\n",
      "Saving model (epoch = 1022, loss = 0.0154)\n",
      "Saving model (epoch = 1023, loss = 0.0154)\n",
      "Saving model (epoch = 1024, loss = 0.0154)\n",
      "Saving model (epoch = 1025, loss = 0.0154)\n",
      "Saving model (epoch = 1027, loss = 0.0154)\n",
      "Saving model (epoch = 1028, loss = 0.0154)\n",
      "Saving model (epoch = 1029, loss = 0.0154)\n",
      "Saving model (epoch = 1031, loss = 0.0154)\n",
      "Saving model (epoch = 1032, loss = 0.0154)\n",
      "Saving model (epoch = 1033, loss = 0.0154)\n",
      "Saving model (epoch = 1035, loss = 0.0154)\n",
      "Saving model (epoch = 1037, loss = 0.0154)\n",
      "Saving model (epoch = 1038, loss = 0.0154)\n",
      "Saving model (epoch = 1039, loss = 0.0154)\n",
      "Saving model (epoch = 1040, loss = 0.0154)\n",
      "Saving model (epoch = 1042, loss = 0.0154)\n",
      "Saving model (epoch = 1043, loss = 0.0154)\n",
      "Saving model (epoch = 1044, loss = 0.0154)\n",
      "Saving model (epoch = 1045, loss = 0.0154)\n",
      "Saving model (epoch = 1046, loss = 0.0154)\n",
      "Saving model (epoch = 1047, loss = 0.0153)\n",
      "Saving model (epoch = 1048, loss = 0.0153)\n",
      "Saving model (epoch = 1050, loss = 0.0153)\n",
      "Saving model (epoch = 1051, loss = 0.0153)\n",
      "Saving model (epoch = 1053, loss = 0.0153)\n",
      "Saving model (epoch = 1054, loss = 0.0153)\n",
      "Saving model (epoch = 1057, loss = 0.0153)\n",
      "Saving model (epoch = 1059, loss = 0.0153)\n",
      "Saving model (epoch = 1061, loss = 0.0153)\n",
      "Saving model (epoch = 1063, loss = 0.0153)\n",
      "Saving model (epoch = 1064, loss = 0.0153)\n",
      "Saving model (epoch = 1065, loss = 0.0153)\n",
      "Saving model (epoch = 1067, loss = 0.0153)\n",
      "Saving model (epoch = 1068, loss = 0.0153)\n",
      "Saving model (epoch = 1069, loss = 0.0153)\n",
      "Saving model (epoch = 1071, loss = 0.0153)\n",
      "Saving model (epoch = 1073, loss = 0.0153)\n",
      "Saving model (epoch = 1074, loss = 0.0153)\n",
      "Saving model (epoch = 1075, loss = 0.0153)\n",
      "Saving model (epoch = 1076, loss = 0.0153)\n",
      "Saving model (epoch = 1079, loss = 0.0153)\n",
      "Saving model (epoch = 1080, loss = 0.0153)\n",
      "Saving model (epoch = 1081, loss = 0.0153)\n",
      "Saving model (epoch = 1082, loss = 0.0152)\n",
      "Saving model (epoch = 1083, loss = 0.0152)\n",
      "Saving model (epoch = 1084, loss = 0.0152)\n",
      "Saving model (epoch = 1085, loss = 0.0152)\n",
      "Saving model (epoch = 1087, loss = 0.0152)\n",
      "Saving model (epoch = 1089, loss = 0.0152)\n",
      "Saving model (epoch = 1091, loss = 0.0152)\n",
      "Saving model (epoch = 1092, loss = 0.0152)\n",
      "Saving model (epoch = 1094, loss = 0.0152)\n",
      "Saving model (epoch = 1095, loss = 0.0152)\n",
      "Saving model (epoch = 1097, loss = 0.0152)\n",
      "Saving model (epoch = 1098, loss = 0.0152)\n",
      "Saving model (epoch = 1099, loss = 0.0152)\n",
      "Saving model (epoch = 1101, loss = 0.0152)\n",
      "Saving model (epoch = 1104, loss = 0.0152)\n",
      "Saving model (epoch = 1105, loss = 0.0152)\n",
      "Saving model (epoch = 1106, loss = 0.0152)\n",
      "Saving model (epoch = 1108, loss = 0.0152)\n",
      "Saving model (epoch = 1109, loss = 0.0152)\n",
      "Saving model (epoch = 1111, loss = 0.0152)\n",
      "Saving model (epoch = 1113, loss = 0.0152)\n",
      "Saving model (epoch = 1115, loss = 0.0152)\n",
      "Saving model (epoch = 1116, loss = 0.0152)\n",
      "Saving model (epoch = 1117, loss = 0.0152)\n",
      "Saving model (epoch = 1118, loss = 0.0152)\n",
      "Saving model (epoch = 1119, loss = 0.0152)\n",
      "Saving model (epoch = 1120, loss = 0.0152)\n",
      "Saving model (epoch = 1121, loss = 0.0151)\n",
      "Saving model (epoch = 1122, loss = 0.0151)\n",
      "Saving model (epoch = 1123, loss = 0.0151)\n",
      "Saving model (epoch = 1125, loss = 0.0151)\n",
      "Saving model (epoch = 1126, loss = 0.0151)\n",
      "Saving model (epoch = 1129, loss = 0.0151)\n",
      "Saving model (epoch = 1130, loss = 0.0151)\n",
      "Saving model (epoch = 1133, loss = 0.0151)\n",
      "Saving model (epoch = 1135, loss = 0.0151)\n",
      "Saving model (epoch = 1136, loss = 0.0151)\n",
      "Saving model (epoch = 1138, loss = 0.0151)\n",
      "Saving model (epoch = 1139, loss = 0.0151)\n",
      "Saving model (epoch = 1140, loss = 0.0151)\n",
      "Saving model (epoch = 1141, loss = 0.0151)\n",
      "Saving model (epoch = 1143, loss = 0.0151)\n",
      "Saving model (epoch = 1144, loss = 0.0151)\n",
      "Saving model (epoch = 1145, loss = 0.0151)\n",
      "Saving model (epoch = 1146, loss = 0.0151)\n",
      "Saving model (epoch = 1147, loss = 0.0151)\n",
      "Saving model (epoch = 1148, loss = 0.0151)\n",
      "Saving model (epoch = 1150, loss = 0.0151)\n",
      "Saving model (epoch = 1151, loss = 0.0151)\n",
      "Saving model (epoch = 1153, loss = 0.0151)\n",
      "Saving model (epoch = 1155, loss = 0.0151)\n",
      "Saving model (epoch = 1156, loss = 0.0151)\n",
      "Saving model (epoch = 1157, loss = 0.0151)\n",
      "Saving model (epoch = 1158, loss = 0.0150)\n",
      "Saving model (epoch = 1160, loss = 0.0150)\n",
      "Saving model (epoch = 1161, loss = 0.0150)\n",
      "Saving model (epoch = 1163, loss = 0.0150)\n",
      "Saving model (epoch = 1164, loss = 0.0150)\n",
      "Saving model (epoch = 1166, loss = 0.0150)\n",
      "Saving model (epoch = 1168, loss = 0.0150)\n",
      "Saving model (epoch = 1173, loss = 0.0150)\n",
      "Saving model (epoch = 1174, loss = 0.0150)\n",
      "Saving model (epoch = 1176, loss = 0.0150)\n",
      "Saving model (epoch = 1177, loss = 0.0150)\n",
      "Saving model (epoch = 1178, loss = 0.0150)\n",
      "Saving model (epoch = 1180, loss = 0.0150)\n",
      "Saving model (epoch = 1181, loss = 0.0150)\n",
      "Saving model (epoch = 1182, loss = 0.0150)\n",
      "Saving model (epoch = 1183, loss = 0.0150)\n",
      "Saving model (epoch = 1184, loss = 0.0150)\n",
      "Saving model (epoch = 1185, loss = 0.0150)\n",
      "Saving model (epoch = 1186, loss = 0.0150)\n",
      "Saving model (epoch = 1187, loss = 0.0150)\n",
      "Saving model (epoch = 1188, loss = 0.0150)\n",
      "Saving model (epoch = 1189, loss = 0.0150)\n",
      "Saving model (epoch = 1190, loss = 0.0150)\n",
      "Saving model (epoch = 1191, loss = 0.0150)\n",
      "Saving model (epoch = 1193, loss = 0.0150)\n",
      "Saving model (epoch = 1195, loss = 0.0150)\n",
      "Saving model (epoch = 1196, loss = 0.0150)\n",
      "Saving model (epoch = 1197, loss = 0.0149)\n",
      "Saving model (epoch = 1199, loss = 0.0149)\n",
      "Saving model (epoch = 1201, loss = 0.0149)\n",
      "Saving model (epoch = 1203, loss = 0.0149)\n",
      "Saving model (epoch = 1205, loss = 0.0149)\n",
      "Saving model (epoch = 1206, loss = 0.0149)\n",
      "Saving model (epoch = 1207, loss = 0.0149)\n",
      "Saving model (epoch = 1208, loss = 0.0149)\n",
      "Saving model (epoch = 1209, loss = 0.0149)\n",
      "Saving model (epoch = 1211, loss = 0.0149)\n",
      "Saving model (epoch = 1213, loss = 0.0149)\n",
      "Saving model (epoch = 1215, loss = 0.0149)\n",
      "Saving model (epoch = 1216, loss = 0.0149)\n",
      "Saving model (epoch = 1217, loss = 0.0149)\n",
      "Saving model (epoch = 1219, loss = 0.0149)\n",
      "Saving model (epoch = 1220, loss = 0.0149)\n",
      "Saving model (epoch = 1222, loss = 0.0149)\n",
      "Saving model (epoch = 1224, loss = 0.0149)\n",
      "Saving model (epoch = 1225, loss = 0.0149)\n",
      "Saving model (epoch = 1226, loss = 0.0149)\n",
      "Saving model (epoch = 1229, loss = 0.0149)\n",
      "Saving model (epoch = 1230, loss = 0.0149)\n",
      "Saving model (epoch = 1234, loss = 0.0149)\n",
      "Saving model (epoch = 1237, loss = 0.0149)\n",
      "Saving model (epoch = 1238, loss = 0.0149)\n",
      "Saving model (epoch = 1242, loss = 0.0148)\n",
      "Saving model (epoch = 1244, loss = 0.0148)\n",
      "Saving model (epoch = 1246, loss = 0.0148)\n",
      "Saving model (epoch = 1247, loss = 0.0148)\n",
      "Saving model (epoch = 1248, loss = 0.0148)\n",
      "Saving model (epoch = 1250, loss = 0.0148)\n",
      "Saving model (epoch = 1251, loss = 0.0148)\n",
      "Saving model (epoch = 1252, loss = 0.0148)\n",
      "Saving model (epoch = 1254, loss = 0.0148)\n",
      "Saving model (epoch = 1257, loss = 0.0148)\n",
      "Saving model (epoch = 1259, loss = 0.0148)\n",
      "Saving model (epoch = 1261, loss = 0.0148)\n",
      "Saving model (epoch = 1262, loss = 0.0148)\n",
      "Saving model (epoch = 1263, loss = 0.0148)\n",
      "Saving model (epoch = 1265, loss = 0.0148)\n",
      "Saving model (epoch = 1268, loss = 0.0148)\n",
      "Saving model (epoch = 1269, loss = 0.0148)\n",
      "Saving model (epoch = 1271, loss = 0.0148)\n",
      "Saving model (epoch = 1275, loss = 0.0148)\n",
      "Saving model (epoch = 1277, loss = 0.0148)\n",
      "Saving model (epoch = 1278, loss = 0.0148)\n",
      "Saving model (epoch = 1279, loss = 0.0148)\n",
      "Saving model (epoch = 1280, loss = 0.0148)\n",
      "Saving model (epoch = 1282, loss = 0.0148)\n",
      "Saving model (epoch = 1283, loss = 0.0148)\n",
      "Saving model (epoch = 1284, loss = 0.0148)\n",
      "Saving model (epoch = 1288, loss = 0.0148)\n",
      "Saving model (epoch = 1289, loss = 0.0148)\n",
      "Saving model (epoch = 1290, loss = 0.0147)\n",
      "Saving model (epoch = 1292, loss = 0.0147)\n",
      "Saving model (epoch = 1295, loss = 0.0147)\n",
      "Saving model (epoch = 1297, loss = 0.0147)\n",
      "Saving model (epoch = 1299, loss = 0.0147)\n",
      "Saving model (epoch = 1300, loss = 0.0147)\n",
      "Saving model (epoch = 1301, loss = 0.0147)\n",
      "Saving model (epoch = 1302, loss = 0.0147)\n",
      "Saving model (epoch = 1304, loss = 0.0147)\n",
      "Saving model (epoch = 1305, loss = 0.0147)\n",
      "Saving model (epoch = 1308, loss = 0.0147)\n",
      "Saving model (epoch = 1309, loss = 0.0147)\n",
      "Saving model (epoch = 1310, loss = 0.0147)\n",
      "Saving model (epoch = 1312, loss = 0.0147)\n",
      "Saving model (epoch = 1316, loss = 0.0147)\n",
      "Saving model (epoch = 1317, loss = 0.0147)\n",
      "Saving model (epoch = 1320, loss = 0.0147)\n",
      "Saving model (epoch = 1321, loss = 0.0147)\n",
      "Saving model (epoch = 1324, loss = 0.0147)\n",
      "Saving model (epoch = 1325, loss = 0.0147)\n",
      "Saving model (epoch = 1326, loss = 0.0147)\n",
      "Saving model (epoch = 1328, loss = 0.0147)\n",
      "Saving model (epoch = 1329, loss = 0.0147)\n",
      "Saving model (epoch = 1330, loss = 0.0147)\n",
      "Saving model (epoch = 1331, loss = 0.0147)\n",
      "Saving model (epoch = 1332, loss = 0.0147)\n",
      "Saving model (epoch = 1333, loss = 0.0147)\n",
      "Saving model (epoch = 1334, loss = 0.0147)\n",
      "Saving model (epoch = 1337, loss = 0.0147)\n",
      "Saving model (epoch = 1339, loss = 0.0147)\n",
      "Saving model (epoch = 1340, loss = 0.0147)\n",
      "Saving model (epoch = 1341, loss = 0.0147)\n",
      "Saving model (epoch = 1342, loss = 0.0146)\n",
      "Saving model (epoch = 1343, loss = 0.0146)\n",
      "Saving model (epoch = 1345, loss = 0.0146)\n",
      "Saving model (epoch = 1347, loss = 0.0146)\n",
      "Saving model (epoch = 1349, loss = 0.0146)\n",
      "Saving model (epoch = 1354, loss = 0.0146)\n",
      "Saving model (epoch = 1355, loss = 0.0146)\n",
      "Saving model (epoch = 1357, loss = 0.0146)\n",
      "Saving model (epoch = 1360, loss = 0.0146)\n",
      "Saving model (epoch = 1363, loss = 0.0146)\n",
      "Saving model (epoch = 1364, loss = 0.0146)\n",
      "Saving model (epoch = 1365, loss = 0.0146)\n",
      "Saving model (epoch = 1367, loss = 0.0146)\n",
      "Saving model (epoch = 1368, loss = 0.0146)\n",
      "Saving model (epoch = 1369, loss = 0.0146)\n",
      "Saving model (epoch = 1370, loss = 0.0146)\n",
      "Saving model (epoch = 1371, loss = 0.0146)\n",
      "Saving model (epoch = 1374, loss = 0.0146)\n",
      "Saving model (epoch = 1375, loss = 0.0146)\n",
      "Saving model (epoch = 1377, loss = 0.0146)\n",
      "Saving model (epoch = 1378, loss = 0.0146)\n",
      "Saving model (epoch = 1379, loss = 0.0146)\n",
      "Saving model (epoch = 1381, loss = 0.0146)\n",
      "Saving model (epoch = 1383, loss = 0.0146)\n",
      "Saving model (epoch = 1384, loss = 0.0146)\n",
      "Saving model (epoch = 1387, loss = 0.0146)\n",
      "Saving model (epoch = 1388, loss = 0.0146)\n",
      "Saving model (epoch = 1389, loss = 0.0146)\n",
      "Saving model (epoch = 1393, loss = 0.0146)\n",
      "Saving model (epoch = 1395, loss = 0.0145)\n",
      "Saving model (epoch = 1396, loss = 0.0145)\n",
      "Saving model (epoch = 1397, loss = 0.0145)\n",
      "Saving model (epoch = 1398, loss = 0.0145)\n",
      "Saving model (epoch = 1400, loss = 0.0145)\n",
      "Saving model (epoch = 1401, loss = 0.0145)\n",
      "Saving model (epoch = 1402, loss = 0.0145)\n",
      "Saving model (epoch = 1403, loss = 0.0145)\n",
      "Saving model (epoch = 1405, loss = 0.0145)\n",
      "Saving model (epoch = 1406, loss = 0.0145)\n",
      "Saving model (epoch = 1408, loss = 0.0145)\n",
      "Saving model (epoch = 1411, loss = 0.0145)\n",
      "Saving model (epoch = 1412, loss = 0.0145)\n",
      "Saving model (epoch = 1414, loss = 0.0145)\n",
      "Saving model (epoch = 1416, loss = 0.0145)\n",
      "Saving model (epoch = 1418, loss = 0.0145)\n",
      "Saving model (epoch = 1419, loss = 0.0145)\n",
      "Saving model (epoch = 1421, loss = 0.0145)\n",
      "Saving model (epoch = 1422, loss = 0.0145)\n",
      "Saving model (epoch = 1423, loss = 0.0145)\n",
      "Saving model (epoch = 1424, loss = 0.0145)\n",
      "Saving model (epoch = 1425, loss = 0.0145)\n",
      "Saving model (epoch = 1428, loss = 0.0145)\n",
      "Saving model (epoch = 1429, loss = 0.0145)\n",
      "Saving model (epoch = 1430, loss = 0.0145)\n",
      "Saving model (epoch = 1431, loss = 0.0145)\n",
      "Saving model (epoch = 1432, loss = 0.0145)\n",
      "Saving model (epoch = 1433, loss = 0.0145)\n",
      "Saving model (epoch = 1435, loss = 0.0145)\n",
      "Saving model (epoch = 1439, loss = 0.0145)\n",
      "Saving model (epoch = 1441, loss = 0.0145)\n",
      "Saving model (epoch = 1442, loss = 0.0145)\n",
      "Saving model (epoch = 1445, loss = 0.0145)\n",
      "Saving model (epoch = 1446, loss = 0.0145)\n",
      "Saving model (epoch = 1449, loss = 0.0145)\n",
      "Saving model (epoch = 1450, loss = 0.0145)\n",
      "Saving model (epoch = 1453, loss = 0.0145)\n",
      "Saving model (epoch = 1455, loss = 0.0144)\n",
      "Saving model (epoch = 1461, loss = 0.0144)\n",
      "Saving model (epoch = 1462, loss = 0.0144)\n",
      "Saving model (epoch = 1463, loss = 0.0144)\n",
      "Saving model (epoch = 1466, loss = 0.0144)\n",
      "Saving model (epoch = 1467, loss = 0.0144)\n",
      "Saving model (epoch = 1470, loss = 0.0144)\n",
      "Saving model (epoch = 1471, loss = 0.0144)\n",
      "Saving model (epoch = 1473, loss = 0.0144)\n",
      "Saving model (epoch = 1476, loss = 0.0144)\n",
      "Saving model (epoch = 1478, loss = 0.0144)\n",
      "Saving model (epoch = 1480, loss = 0.0144)\n",
      "Saving model (epoch = 1482, loss = 0.0144)\n",
      "Saving model (epoch = 1483, loss = 0.0144)\n",
      "Saving model (epoch = 1484, loss = 0.0144)\n",
      "Saving model (epoch = 1487, loss = 0.0144)\n",
      "Saving model (epoch = 1488, loss = 0.0144)\n",
      "Saving model (epoch = 1494, loss = 0.0144)\n",
      "Saving model (epoch = 1495, loss = 0.0144)\n",
      "Saving model (epoch = 1496, loss = 0.0144)\n",
      "Saving model (epoch = 1499, loss = 0.0144)\n",
      "Saving model (epoch = 1502, loss = 0.0144)\n",
      "Saving model (epoch = 1505, loss = 0.0144)\n",
      "Saving model (epoch = 1506, loss = 0.0144)\n",
      "Saving model (epoch = 1507, loss = 0.0144)\n",
      "Saving model (epoch = 1511, loss = 0.0144)\n",
      "Saving model (epoch = 1514, loss = 0.0144)\n",
      "Saving model (epoch = 1517, loss = 0.0144)\n",
      "Saving model (epoch = 1519, loss = 0.0143)\n",
      "Saving model (epoch = 1520, loss = 0.0143)\n",
      "Saving model (epoch = 1522, loss = 0.0143)\n",
      "Saving model (epoch = 1523, loss = 0.0143)\n",
      "Saving model (epoch = 1524, loss = 0.0143)\n",
      "Saving model (epoch = 1527, loss = 0.0143)\n",
      "Saving model (epoch = 1528, loss = 0.0143)\n",
      "Saving model (epoch = 1530, loss = 0.0143)\n",
      "Saving model (epoch = 1531, loss = 0.0143)\n",
      "Saving model (epoch = 1533, loss = 0.0143)\n",
      "Saving model (epoch = 1535, loss = 0.0143)\n",
      "Saving model (epoch = 1536, loss = 0.0143)\n",
      "Saving model (epoch = 1537, loss = 0.0143)\n",
      "Saving model (epoch = 1538, loss = 0.0143)\n",
      "Saving model (epoch = 1541, loss = 0.0143)\n",
      "Saving model (epoch = 1543, loss = 0.0143)\n",
      "Saving model (epoch = 1544, loss = 0.0143)\n",
      "Saving model (epoch = 1545, loss = 0.0143)\n",
      "Saving model (epoch = 1548, loss = 0.0143)\n",
      "Saving model (epoch = 1549, loss = 0.0143)\n",
      "Saving model (epoch = 1552, loss = 0.0143)\n",
      "Saving model (epoch = 1554, loss = 0.0143)\n",
      "Saving model (epoch = 1555, loss = 0.0143)\n",
      "Saving model (epoch = 1556, loss = 0.0143)\n",
      "Saving model (epoch = 1558, loss = 0.0143)\n",
      "Saving model (epoch = 1559, loss = 0.0143)\n",
      "Saving model (epoch = 1560, loss = 0.0143)\n",
      "Saving model (epoch = 1561, loss = 0.0143)\n",
      "Saving model (epoch = 1562, loss = 0.0143)\n",
      "Saving model (epoch = 1564, loss = 0.0143)\n",
      "Saving model (epoch = 1565, loss = 0.0143)\n",
      "Saving model (epoch = 1568, loss = 0.0143)\n",
      "Saving model (epoch = 1569, loss = 0.0143)\n",
      "Saving model (epoch = 1570, loss = 0.0143)\n",
      "Saving model (epoch = 1571, loss = 0.0143)\n",
      "Saving model (epoch = 1575, loss = 0.0143)\n",
      "Saving model (epoch = 1576, loss = 0.0143)\n",
      "Saving model (epoch = 1577, loss = 0.0143)\n",
      "Saving model (epoch = 1582, loss = 0.0143)\n",
      "Saving model (epoch = 1583, loss = 0.0143)\n",
      "Saving model (epoch = 1585, loss = 0.0142)\n",
      "Saving model (epoch = 1588, loss = 0.0142)\n",
      "Saving model (epoch = 1589, loss = 0.0142)\n",
      "Saving model (epoch = 1591, loss = 0.0142)\n",
      "Saving model (epoch = 1593, loss = 0.0142)\n",
      "Saving model (epoch = 1594, loss = 0.0142)\n",
      "Saving model (epoch = 1595, loss = 0.0142)\n",
      "Saving model (epoch = 1597, loss = 0.0142)\n",
      "Saving model (epoch = 1601, loss = 0.0142)\n",
      "Saving model (epoch = 1602, loss = 0.0142)\n",
      "Saving model (epoch = 1603, loss = 0.0142)\n",
      "Saving model (epoch = 1604, loss = 0.0142)\n",
      "Saving model (epoch = 1607, loss = 0.0142)\n",
      "Saving model (epoch = 1611, loss = 0.0142)\n",
      "Saving model (epoch = 1615, loss = 0.0142)\n",
      "Saving model (epoch = 1617, loss = 0.0142)\n",
      "Saving model (epoch = 1618, loss = 0.0142)\n",
      "Saving model (epoch = 1623, loss = 0.0142)\n",
      "Saving model (epoch = 1627, loss = 0.0142)\n",
      "Saving model (epoch = 1628, loss = 0.0142)\n",
      "Saving model (epoch = 1631, loss = 0.0142)\n",
      "Saving model (epoch = 1632, loss = 0.0142)\n",
      "Saving model (epoch = 1633, loss = 0.0142)\n",
      "Saving model (epoch = 1636, loss = 0.0142)\n",
      "Saving model (epoch = 1637, loss = 0.0142)\n",
      "Saving model (epoch = 1638, loss = 0.0142)\n",
      "Saving model (epoch = 1642, loss = 0.0142)\n",
      "Saving model (epoch = 1647, loss = 0.0142)\n",
      "Saving model (epoch = 1650, loss = 0.0142)\n",
      "Saving model (epoch = 1653, loss = 0.0142)\n",
      "Saving model (epoch = 1654, loss = 0.0142)\n",
      "Saving model (epoch = 1658, loss = 0.0142)\n",
      "Saving model (epoch = 1659, loss = 0.0141)\n",
      "Saving model (epoch = 1661, loss = 0.0141)\n",
      "Saving model (epoch = 1662, loss = 0.0141)\n",
      "Saving model (epoch = 1665, loss = 0.0141)\n",
      "Saving model (epoch = 1666, loss = 0.0141)\n",
      "Saving model (epoch = 1667, loss = 0.0141)\n",
      "Saving model (epoch = 1668, loss = 0.0141)\n",
      "Saving model (epoch = 1669, loss = 0.0141)\n",
      "Saving model (epoch = 1675, loss = 0.0141)\n",
      "Saving model (epoch = 1676, loss = 0.0141)\n",
      "Saving model (epoch = 1677, loss = 0.0141)\n",
      "Saving model (epoch = 1680, loss = 0.0141)\n",
      "Saving model (epoch = 1681, loss = 0.0141)\n",
      "Saving model (epoch = 1684, loss = 0.0141)\n",
      "Saving model (epoch = 1685, loss = 0.0141)\n",
      "Saving model (epoch = 1686, loss = 0.0141)\n",
      "Saving model (epoch = 1688, loss = 0.0141)\n",
      "Saving model (epoch = 1689, loss = 0.0141)\n",
      "Saving model (epoch = 1692, loss = 0.0141)\n",
      "Saving model (epoch = 1694, loss = 0.0141)\n",
      "Saving model (epoch = 1697, loss = 0.0141)\n",
      "Saving model (epoch = 1700, loss = 0.0141)\n",
      "Saving model (epoch = 1707, loss = 0.0141)\n",
      "Saving model (epoch = 1710, loss = 0.0141)\n",
      "Saving model (epoch = 1712, loss = 0.0141)\n",
      "Saving model (epoch = 1716, loss = 0.0141)\n",
      "Saving model (epoch = 1719, loss = 0.0141)\n",
      "Saving model (epoch = 1722, loss = 0.0141)\n",
      "Saving model (epoch = 1725, loss = 0.0141)\n",
      "Saving model (epoch = 1726, loss = 0.0141)\n",
      "Saving model (epoch = 1728, loss = 0.0141)\n",
      "Saving model (epoch = 1730, loss = 0.0141)\n",
      "Saving model (epoch = 1735, loss = 0.0140)\n",
      "Saving model (epoch = 1736, loss = 0.0140)\n",
      "Saving model (epoch = 1739, loss = 0.0140)\n",
      "Saving model (epoch = 1744, loss = 0.0140)\n",
      "Saving model (epoch = 1747, loss = 0.0140)\n",
      "Saving model (epoch = 1751, loss = 0.0140)\n",
      "Saving model (epoch = 1752, loss = 0.0140)\n",
      "Saving model (epoch = 1754, loss = 0.0140)\n",
      "Saving model (epoch = 1756, loss = 0.0140)\n",
      "Saving model (epoch = 1758, loss = 0.0140)\n",
      "Saving model (epoch = 1759, loss = 0.0140)\n",
      "Saving model (epoch = 1764, loss = 0.0140)\n",
      "Saving model (epoch = 1766, loss = 0.0140)\n",
      "Saving model (epoch = 1767, loss = 0.0140)\n",
      "Saving model (epoch = 1770, loss = 0.0140)\n",
      "Saving model (epoch = 1771, loss = 0.0140)\n",
      "Saving model (epoch = 1772, loss = 0.0140)\n",
      "Saving model (epoch = 1774, loss = 0.0140)\n",
      "Saving model (epoch = 1777, loss = 0.0140)\n",
      "Saving model (epoch = 1779, loss = 0.0140)\n",
      "Saving model (epoch = 1781, loss = 0.0140)\n",
      "Saving model (epoch = 1782, loss = 0.0140)\n",
      "Saving model (epoch = 1785, loss = 0.0140)\n",
      "Saving model (epoch = 1790, loss = 0.0140)\n",
      "Saving model (epoch = 1791, loss = 0.0140)\n",
      "Saving model (epoch = 1795, loss = 0.0140)\n",
      "Saving model (epoch = 1796, loss = 0.0140)\n",
      "Saving model (epoch = 1799, loss = 0.0140)\n",
      "Saving model (epoch = 1801, loss = 0.0140)\n",
      "Saving model (epoch = 1803, loss = 0.0140)\n",
      "Saving model (epoch = 1804, loss = 0.0140)\n",
      "Saving model (epoch = 1805, loss = 0.0140)\n",
      "Saving model (epoch = 1807, loss = 0.0140)\n",
      "Saving model (epoch = 1808, loss = 0.0140)\n",
      "Saving model (epoch = 1811, loss = 0.0140)\n",
      "Saving model (epoch = 1815, loss = 0.0140)\n",
      "Saving model (epoch = 1816, loss = 0.0140)\n",
      "Saving model (epoch = 1817, loss = 0.0140)\n",
      "Saving model (epoch = 1820, loss = 0.0140)\n",
      "Saving model (epoch = 1823, loss = 0.0139)\n",
      "Saving model (epoch = 1825, loss = 0.0139)\n",
      "Saving model (epoch = 1828, loss = 0.0139)\n",
      "Saving model (epoch = 1829, loss = 0.0139)\n",
      "Saving model (epoch = 1830, loss = 0.0139)\n",
      "Saving model (epoch = 1831, loss = 0.0139)\n",
      "Saving model (epoch = 1832, loss = 0.0139)\n",
      "Saving model (epoch = 1833, loss = 0.0139)\n",
      "Saving model (epoch = 1835, loss = 0.0139)\n",
      "Saving model (epoch = 1837, loss = 0.0139)\n",
      "Saving model (epoch = 1841, loss = 0.0139)\n",
      "Saving model (epoch = 1842, loss = 0.0139)\n",
      "Saving model (epoch = 1844, loss = 0.0139)\n",
      "Saving model (epoch = 1845, loss = 0.0139)\n",
      "Saving model (epoch = 1848, loss = 0.0139)\n",
      "Saving model (epoch = 1849, loss = 0.0139)\n",
      "Saving model (epoch = 1850, loss = 0.0139)\n",
      "Saving model (epoch = 1851, loss = 0.0139)\n",
      "Saving model (epoch = 1853, loss = 0.0139)\n",
      "Saving model (epoch = 1855, loss = 0.0139)\n",
      "Saving model (epoch = 1856, loss = 0.0139)\n",
      "Saving model (epoch = 1859, loss = 0.0139)\n",
      "Saving model (epoch = 1860, loss = 0.0139)\n",
      "Saving model (epoch = 1861, loss = 0.0139)\n",
      "Saving model (epoch = 1863, loss = 0.0139)\n",
      "Saving model (epoch = 1865, loss = 0.0139)\n",
      "Saving model (epoch = 1867, loss = 0.0139)\n",
      "Saving model (epoch = 1870, loss = 0.0139)\n",
      "Saving model (epoch = 1873, loss = 0.0139)\n",
      "Saving model (epoch = 1874, loss = 0.0139)\n",
      "Saving model (epoch = 1875, loss = 0.0139)\n",
      "Saving model (epoch = 1883, loss = 0.0139)\n",
      "Saving model (epoch = 1885, loss = 0.0139)\n",
      "Saving model (epoch = 1886, loss = 0.0139)\n",
      "Saving model (epoch = 1889, loss = 0.0139)\n",
      "Saving model (epoch = 1891, loss = 0.0139)\n",
      "Saving model (epoch = 1893, loss = 0.0139)\n",
      "Saving model (epoch = 1894, loss = 0.0139)\n",
      "Saving model (epoch = 1896, loss = 0.0139)\n",
      "Saving model (epoch = 1897, loss = 0.0139)\n",
      "Saving model (epoch = 1899, loss = 0.0139)\n",
      "Saving model (epoch = 1905, loss = 0.0138)\n",
      "Saving model (epoch = 1908, loss = 0.0138)\n",
      "Saving model (epoch = 1912, loss = 0.0138)\n",
      "Saving model (epoch = 1913, loss = 0.0138)\n",
      "Saving model (epoch = 1914, loss = 0.0138)\n",
      "Saving model (epoch = 1919, loss = 0.0138)\n",
      "Saving model (epoch = 1920, loss = 0.0138)\n",
      "Saving model (epoch = 1924, loss = 0.0138)\n",
      "Saving model (epoch = 1925, loss = 0.0138)\n",
      "Saving model (epoch = 1927, loss = 0.0138)\n",
      "Saving model (epoch = 1930, loss = 0.0138)\n",
      "Saving model (epoch = 1937, loss = 0.0138)\n",
      "Saving model (epoch = 1939, loss = 0.0138)\n",
      "Saving model (epoch = 1940, loss = 0.0138)\n",
      "Saving model (epoch = 1941, loss = 0.0138)\n",
      "Saving model (epoch = 1942, loss = 0.0138)\n",
      "Saving model (epoch = 1943, loss = 0.0138)\n",
      "Saving model (epoch = 1944, loss = 0.0138)\n",
      "Saving model (epoch = 1945, loss = 0.0138)\n",
      "Saving model (epoch = 1947, loss = 0.0138)\n",
      "Saving model (epoch = 1950, loss = 0.0138)\n",
      "Saving model (epoch = 1955, loss = 0.0138)\n",
      "Saving model (epoch = 1957, loss = 0.0138)\n",
      "Saving model (epoch = 1958, loss = 0.0138)\n",
      "Saving model (epoch = 1959, loss = 0.0138)\n",
      "Saving model (epoch = 1960, loss = 0.0138)\n",
      "Saving model (epoch = 1962, loss = 0.0138)\n",
      "Saving model (epoch = 1966, loss = 0.0138)\n",
      "Saving model (epoch = 1968, loss = 0.0138)\n",
      "Saving model (epoch = 1969, loss = 0.0138)\n",
      "Saving model (epoch = 1971, loss = 0.0138)\n",
      "Saving model (epoch = 1973, loss = 0.0138)\n",
      "Saving model (epoch = 1974, loss = 0.0138)\n",
      "Saving model (epoch = 1976, loss = 0.0138)\n",
      "Saving model (epoch = 1977, loss = 0.0138)\n",
      "Saving model (epoch = 1983, loss = 0.0138)\n",
      "Saving model (epoch = 1984, loss = 0.0138)\n",
      "Saving model (epoch = 1985, loss = 0.0138)\n",
      "Saving model (epoch = 1992, loss = 0.0137)\n",
      "Saving model (epoch = 1996, loss = 0.0137)\n",
      "Saving model (epoch = 1999, loss = 0.0137)\n",
      "Saving model (epoch = 2002, loss = 0.0137)\n",
      "Saving model (epoch = 2004, loss = 0.0137)\n",
      "Saving model (epoch = 2005, loss = 0.0137)\n",
      "Saving model (epoch = 2009, loss = 0.0137)\n",
      "Saving model (epoch = 2010, loss = 0.0137)\n",
      "Saving model (epoch = 2011, loss = 0.0137)\n",
      "Saving model (epoch = 2013, loss = 0.0137)\n",
      "Saving model (epoch = 2014, loss = 0.0137)\n",
      "Saving model (epoch = 2016, loss = 0.0137)\n",
      "Saving model (epoch = 2020, loss = 0.0137)\n",
      "Saving model (epoch = 2021, loss = 0.0137)\n",
      "Saving model (epoch = 2022, loss = 0.0137)\n",
      "Saving model (epoch = 2023, loss = 0.0137)\n",
      "Saving model (epoch = 2029, loss = 0.0137)\n",
      "Saving model (epoch = 2031, loss = 0.0137)\n",
      "Saving model (epoch = 2033, loss = 0.0137)\n",
      "Saving model (epoch = 2034, loss = 0.0137)\n",
      "Saving model (epoch = 2037, loss = 0.0137)\n",
      "Saving model (epoch = 2044, loss = 0.0137)\n",
      "Saving model (epoch = 2047, loss = 0.0137)\n",
      "Saving model (epoch = 2048, loss = 0.0137)\n",
      "Saving model (epoch = 2052, loss = 0.0137)\n",
      "Saving model (epoch = 2055, loss = 0.0137)\n",
      "Saving model (epoch = 2056, loss = 0.0137)\n",
      "Saving model (epoch = 2062, loss = 0.0137)\n",
      "Saving model (epoch = 2063, loss = 0.0137)\n",
      "Saving model (epoch = 2064, loss = 0.0137)\n",
      "Saving model (epoch = 2066, loss = 0.0137)\n",
      "Saving model (epoch = 2073, loss = 0.0137)\n",
      "Saving model (epoch = 2076, loss = 0.0137)\n",
      "Saving model (epoch = 2077, loss = 0.0137)\n",
      "Saving model (epoch = 2079, loss = 0.0137)\n",
      "Saving model (epoch = 2083, loss = 0.0137)\n",
      "Saving model (epoch = 2084, loss = 0.0136)\n",
      "Saving model (epoch = 2087, loss = 0.0136)\n",
      "Saving model (epoch = 2095, loss = 0.0136)\n",
      "Saving model (epoch = 2096, loss = 0.0136)\n",
      "Saving model (epoch = 2100, loss = 0.0136)\n",
      "Saving model (epoch = 2101, loss = 0.0136)\n",
      "Saving model (epoch = 2104, loss = 0.0136)\n",
      "Saving model (epoch = 2107, loss = 0.0136)\n",
      "Saving model (epoch = 2108, loss = 0.0136)\n",
      "Saving model (epoch = 2115, loss = 0.0136)\n",
      "Saving model (epoch = 2117, loss = 0.0136)\n",
      "Saving model (epoch = 2119, loss = 0.0136)\n",
      "Saving model (epoch = 2120, loss = 0.0136)\n",
      "Saving model (epoch = 2122, loss = 0.0136)\n",
      "Saving model (epoch = 2127, loss = 0.0136)\n",
      "Saving model (epoch = 2130, loss = 0.0136)\n",
      "Saving model (epoch = 2132, loss = 0.0136)\n",
      "Saving model (epoch = 2133, loss = 0.0136)\n",
      "Saving model (epoch = 2137, loss = 0.0136)\n",
      "Saving model (epoch = 2142, loss = 0.0136)\n",
      "Saving model (epoch = 2146, loss = 0.0136)\n",
      "Saving model (epoch = 2151, loss = 0.0136)\n",
      "Saving model (epoch = 2153, loss = 0.0136)\n",
      "Saving model (epoch = 2154, loss = 0.0136)\n",
      "Saving model (epoch = 2155, loss = 0.0136)\n",
      "Saving model (epoch = 2158, loss = 0.0136)\n",
      "Saving model (epoch = 2159, loss = 0.0136)\n",
      "Saving model (epoch = 2161, loss = 0.0136)\n",
      "Saving model (epoch = 2163, loss = 0.0136)\n",
      "Saving model (epoch = 2165, loss = 0.0136)\n",
      "Saving model (epoch = 2167, loss = 0.0136)\n",
      "Saving model (epoch = 2168, loss = 0.0136)\n",
      "Saving model (epoch = 2169, loss = 0.0136)\n",
      "Saving model (epoch = 2171, loss = 0.0136)\n",
      "Saving model (epoch = 2172, loss = 0.0136)\n",
      "Saving model (epoch = 2174, loss = 0.0136)\n",
      "Saving model (epoch = 2175, loss = 0.0136)\n",
      "Saving model (epoch = 2177, loss = 0.0136)\n",
      "Saving model (epoch = 2180, loss = 0.0136)\n",
      "Saving model (epoch = 2184, loss = 0.0136)\n",
      "Saving model (epoch = 2185, loss = 0.0135)\n",
      "Saving model (epoch = 2186, loss = 0.0135)\n",
      "Saving model (epoch = 2188, loss = 0.0135)\n",
      "Saving model (epoch = 2189, loss = 0.0135)\n",
      "Saving model (epoch = 2193, loss = 0.0135)\n",
      "Saving model (epoch = 2195, loss = 0.0135)\n",
      "Saving model (epoch = 2197, loss = 0.0135)\n",
      "Saving model (epoch = 2203, loss = 0.0135)\n",
      "Saving model (epoch = 2205, loss = 0.0135)\n",
      "Saving model (epoch = 2213, loss = 0.0135)\n",
      "Saving model (epoch = 2215, loss = 0.0135)\n",
      "Saving model (epoch = 2220, loss = 0.0135)\n",
      "Saving model (epoch = 2221, loss = 0.0135)\n",
      "Saving model (epoch = 2222, loss = 0.0135)\n",
      "Saving model (epoch = 2227, loss = 0.0135)\n",
      "Saving model (epoch = 2228, loss = 0.0135)\n",
      "Saving model (epoch = 2230, loss = 0.0135)\n",
      "Saving model (epoch = 2233, loss = 0.0135)\n",
      "Saving model (epoch = 2236, loss = 0.0135)\n",
      "Saving model (epoch = 2238, loss = 0.0135)\n",
      "Saving model (epoch = 2240, loss = 0.0135)\n",
      "Saving model (epoch = 2242, loss = 0.0135)\n",
      "Saving model (epoch = 2243, loss = 0.0135)\n",
      "Saving model (epoch = 2245, loss = 0.0135)\n",
      "Saving model (epoch = 2246, loss = 0.0135)\n",
      "Saving model (epoch = 2248, loss = 0.0135)\n",
      "Saving model (epoch = 2249, loss = 0.0135)\n",
      "Saving model (epoch = 2250, loss = 0.0135)\n",
      "Saving model (epoch = 2251, loss = 0.0135)\n",
      "Saving model (epoch = 2252, loss = 0.0135)\n",
      "Saving model (epoch = 2254, loss = 0.0135)\n",
      "Saving model (epoch = 2258, loss = 0.0135)\n",
      "Saving model (epoch = 2259, loss = 0.0135)\n",
      "Saving model (epoch = 2261, loss = 0.0135)\n",
      "Saving model (epoch = 2262, loss = 0.0135)\n",
      "Saving model (epoch = 2263, loss = 0.0135)\n",
      "Saving model (epoch = 2267, loss = 0.0135)\n",
      "Saving model (epoch = 2268, loss = 0.0135)\n",
      "Saving model (epoch = 2272, loss = 0.0135)\n",
      "Saving model (epoch = 2274, loss = 0.0135)\n",
      "Saving model (epoch = 2278, loss = 0.0135)\n",
      "Saving model (epoch = 2280, loss = 0.0135)\n",
      "Saving model (epoch = 2284, loss = 0.0135)\n",
      "Saving model (epoch = 2289, loss = 0.0134)\n",
      "Saving model (epoch = 2290, loss = 0.0134)\n",
      "Saving model (epoch = 2292, loss = 0.0134)\n",
      "Saving model (epoch = 2293, loss = 0.0134)\n",
      "Saving model (epoch = 2296, loss = 0.0134)\n",
      "Saving model (epoch = 2297, loss = 0.0134)\n",
      "Saving model (epoch = 2301, loss = 0.0134)\n",
      "Saving model (epoch = 2306, loss = 0.0134)\n",
      "Saving model (epoch = 2307, loss = 0.0134)\n",
      "Saving model (epoch = 2310, loss = 0.0134)\n",
      "Saving model (epoch = 2311, loss = 0.0134)\n",
      "Saving model (epoch = 2314, loss = 0.0134)\n",
      "Saving model (epoch = 2318, loss = 0.0134)\n",
      "Saving model (epoch = 2320, loss = 0.0134)\n",
      "Saving model (epoch = 2325, loss = 0.0134)\n",
      "Saving model (epoch = 2327, loss = 0.0134)\n",
      "Saving model (epoch = 2330, loss = 0.0134)\n",
      "Saving model (epoch = 2331, loss = 0.0134)\n",
      "Saving model (epoch = 2332, loss = 0.0134)\n",
      "Saving model (epoch = 2333, loss = 0.0134)\n",
      "Saving model (epoch = 2334, loss = 0.0134)\n",
      "Saving model (epoch = 2342, loss = 0.0134)\n",
      "Saving model (epoch = 2343, loss = 0.0134)\n",
      "Saving model (epoch = 2346, loss = 0.0134)\n",
      "Saving model (epoch = 2347, loss = 0.0134)\n",
      "Saving model (epoch = 2350, loss = 0.0134)\n",
      "Saving model (epoch = 2351, loss = 0.0134)\n",
      "Saving model (epoch = 2353, loss = 0.0134)\n",
      "Saving model (epoch = 2357, loss = 0.0134)\n",
      "Saving model (epoch = 2358, loss = 0.0134)\n",
      "Saving model (epoch = 2359, loss = 0.0134)\n",
      "Saving model (epoch = 2361, loss = 0.0134)\n",
      "Saving model (epoch = 2362, loss = 0.0134)\n",
      "Saving model (epoch = 2367, loss = 0.0134)\n",
      "Saving model (epoch = 2368, loss = 0.0134)\n",
      "Saving model (epoch = 2372, loss = 0.0134)\n",
      "Saving model (epoch = 2373, loss = 0.0134)\n",
      "Saving model (epoch = 2374, loss = 0.0134)\n",
      "Saving model (epoch = 2375, loss = 0.0134)\n",
      "Saving model (epoch = 2378, loss = 0.0134)\n",
      "Saving model (epoch = 2379, loss = 0.0134)\n",
      "Saving model (epoch = 2384, loss = 0.0134)\n",
      "Saving model (epoch = 2386, loss = 0.0134)\n",
      "Saving model (epoch = 2388, loss = 0.0134)\n",
      "Saving model (epoch = 2391, loss = 0.0134)\n",
      "Saving model (epoch = 2394, loss = 0.0134)\n",
      "Saving model (epoch = 2396, loss = 0.0133)\n",
      "Saving model (epoch = 2398, loss = 0.0133)\n",
      "Saving model (epoch = 2400, loss = 0.0133)\n",
      "Saving model (epoch = 2402, loss = 0.0133)\n",
      "Saving model (epoch = 2403, loss = 0.0133)\n",
      "Saving model (epoch = 2409, loss = 0.0133)\n",
      "Saving model (epoch = 2410, loss = 0.0133)\n",
      "Saving model (epoch = 2413, loss = 0.0133)\n",
      "Saving model (epoch = 2414, loss = 0.0133)\n",
      "Saving model (epoch = 2417, loss = 0.0133)\n",
      "Saving model (epoch = 2421, loss = 0.0133)\n",
      "Saving model (epoch = 2425, loss = 0.0133)\n",
      "Saving model (epoch = 2427, loss = 0.0133)\n",
      "Saving model (epoch = 2428, loss = 0.0133)\n",
      "Saving model (epoch = 2429, loss = 0.0133)\n",
      "Saving model (epoch = 2433, loss = 0.0133)\n",
      "Saving model (epoch = 2436, loss = 0.0133)\n",
      "Saving model (epoch = 2437, loss = 0.0133)\n",
      "Saving model (epoch = 2438, loss = 0.0133)\n",
      "Saving model (epoch = 2440, loss = 0.0133)\n",
      "Saving model (epoch = 2441, loss = 0.0133)\n",
      "Saving model (epoch = 2443, loss = 0.0133)\n",
      "Saving model (epoch = 2444, loss = 0.0133)\n",
      "Saving model (epoch = 2450, loss = 0.0133)\n",
      "Saving model (epoch = 2454, loss = 0.0133)\n",
      "Saving model (epoch = 2456, loss = 0.0133)\n",
      "Saving model (epoch = 2459, loss = 0.0133)\n",
      "Saving model (epoch = 2460, loss = 0.0133)\n",
      "Saving model (epoch = 2461, loss = 0.0133)\n",
      "Saving model (epoch = 2462, loss = 0.0133)\n",
      "Saving model (epoch = 2465, loss = 0.0133)\n",
      "Saving model (epoch = 2466, loss = 0.0133)\n",
      "Saving model (epoch = 2471, loss = 0.0133)\n",
      "Saving model (epoch = 2472, loss = 0.0133)\n",
      "Saving model (epoch = 2475, loss = 0.0133)\n",
      "Saving model (epoch = 2476, loss = 0.0133)\n",
      "Saving model (epoch = 2478, loss = 0.0133)\n",
      "Saving model (epoch = 2481, loss = 0.0133)\n",
      "Saving model (epoch = 2487, loss = 0.0133)\n",
      "Saving model (epoch = 2488, loss = 0.0133)\n",
      "Saving model (epoch = 2492, loss = 0.0133)\n",
      "Saving model (epoch = 2497, loss = 0.0133)\n",
      "Saving model (epoch = 2499, loss = 0.0133)\n",
      "Saving model (epoch = 2500, loss = 0.0133)\n",
      "Saving model (epoch = 2504, loss = 0.0132)\n",
      "Saving model (epoch = 2508, loss = 0.0132)\n",
      "Saving model (epoch = 2509, loss = 0.0132)\n",
      "Saving model (epoch = 2512, loss = 0.0132)\n",
      "Saving model (epoch = 2517, loss = 0.0132)\n",
      "Saving model (epoch = 2519, loss = 0.0132)\n",
      "Saving model (epoch = 2522, loss = 0.0132)\n",
      "Saving model (epoch = 2523, loss = 0.0132)\n",
      "Saving model (epoch = 2527, loss = 0.0132)\n",
      "Saving model (epoch = 2528, loss = 0.0132)\n",
      "Saving model (epoch = 2530, loss = 0.0132)\n",
      "Saving model (epoch = 2534, loss = 0.0132)\n",
      "Saving model (epoch = 2536, loss = 0.0132)\n",
      "Saving model (epoch = 2543, loss = 0.0132)\n",
      "Saving model (epoch = 2547, loss = 0.0132)\n",
      "Saving model (epoch = 2548, loss = 0.0132)\n",
      "Saving model (epoch = 2551, loss = 0.0132)\n",
      "Saving model (epoch = 2552, loss = 0.0132)\n",
      "Saving model (epoch = 2553, loss = 0.0132)\n",
      "Saving model (epoch = 2554, loss = 0.0132)\n",
      "Saving model (epoch = 2557, loss = 0.0132)\n",
      "Saving model (epoch = 2560, loss = 0.0132)\n",
      "Saving model (epoch = 2564, loss = 0.0132)\n",
      "Saving model (epoch = 2566, loss = 0.0132)\n",
      "Saving model (epoch = 2567, loss = 0.0132)\n",
      "Saving model (epoch = 2568, loss = 0.0132)\n",
      "Saving model (epoch = 2569, loss = 0.0132)\n",
      "Saving model (epoch = 2572, loss = 0.0132)\n",
      "Saving model (epoch = 2576, loss = 0.0132)\n",
      "Saving model (epoch = 2577, loss = 0.0132)\n",
      "Saving model (epoch = 2579, loss = 0.0132)\n",
      "Saving model (epoch = 2580, loss = 0.0132)\n",
      "Saving model (epoch = 2581, loss = 0.0132)\n",
      "Saving model (epoch = 2583, loss = 0.0132)\n",
      "Saving model (epoch = 2586, loss = 0.0132)\n",
      "Saving model (epoch = 2590, loss = 0.0132)\n",
      "Saving model (epoch = 2591, loss = 0.0132)\n",
      "Saving model (epoch = 2592, loss = 0.0132)\n",
      "Saving model (epoch = 2593, loss = 0.0132)\n",
      "Saving model (epoch = 2594, loss = 0.0132)\n",
      "Saving model (epoch = 2595, loss = 0.0132)\n",
      "Saving model (epoch = 2599, loss = 0.0132)\n",
      "Saving model (epoch = 2600, loss = 0.0132)\n",
      "Saving model (epoch = 2601, loss = 0.0132)\n",
      "Saving model (epoch = 2607, loss = 0.0132)\n",
      "Saving model (epoch = 2608, loss = 0.0132)\n",
      "Saving model (epoch = 2609, loss = 0.0132)\n",
      "Saving model (epoch = 2610, loss = 0.0132)\n",
      "Saving model (epoch = 2613, loss = 0.0132)\n",
      "Saving model (epoch = 2614, loss = 0.0132)\n",
      "Saving model (epoch = 2617, loss = 0.0132)\n",
      "Saving model (epoch = 2620, loss = 0.0132)\n",
      "Saving model (epoch = 2622, loss = 0.0132)\n",
      "Saving model (epoch = 2623, loss = 0.0132)\n",
      "Saving model (epoch = 2626, loss = 0.0131)\n",
      "Saving model (epoch = 2630, loss = 0.0131)\n",
      "Saving model (epoch = 2633, loss = 0.0131)\n",
      "Saving model (epoch = 2634, loss = 0.0131)\n",
      "Saving model (epoch = 2635, loss = 0.0131)\n",
      "Saving model (epoch = 2639, loss = 0.0131)\n",
      "Saving model (epoch = 2642, loss = 0.0131)\n",
      "Saving model (epoch = 2643, loss = 0.0131)\n",
      "Saving model (epoch = 2648, loss = 0.0131)\n",
      "Saving model (epoch = 2653, loss = 0.0131)\n",
      "Saving model (epoch = 2655, loss = 0.0131)\n",
      "Saving model (epoch = 2656, loss = 0.0131)\n",
      "Saving model (epoch = 2658, loss = 0.0131)\n",
      "Saving model (epoch = 2664, loss = 0.0131)\n",
      "Saving model (epoch = 2666, loss = 0.0131)\n",
      "Saving model (epoch = 2669, loss = 0.0131)\n",
      "Saving model (epoch = 2670, loss = 0.0131)\n",
      "Saving model (epoch = 2672, loss = 0.0131)\n",
      "Saving model (epoch = 2674, loss = 0.0131)\n",
      "Saving model (epoch = 2677, loss = 0.0131)\n",
      "Saving model (epoch = 2678, loss = 0.0131)\n",
      "Saving model (epoch = 2680, loss = 0.0131)\n",
      "Saving model (epoch = 2683, loss = 0.0131)\n",
      "Saving model (epoch = 2687, loss = 0.0131)\n",
      "Saving model (epoch = 2688, loss = 0.0131)\n",
      "Saving model (epoch = 2690, loss = 0.0131)\n",
      "Saving model (epoch = 2693, loss = 0.0131)\n",
      "Saving model (epoch = 2694, loss = 0.0131)\n",
      "Saving model (epoch = 2695, loss = 0.0131)\n",
      "Saving model (epoch = 2699, loss = 0.0131)\n",
      "Saving model (epoch = 2708, loss = 0.0131)\n",
      "Saving model (epoch = 2709, loss = 0.0131)\n",
      "Saving model (epoch = 2710, loss = 0.0131)\n",
      "Saving model (epoch = 2713, loss = 0.0131)\n",
      "Saving model (epoch = 2714, loss = 0.0131)\n",
      "Saving model (epoch = 2719, loss = 0.0131)\n",
      "Saving model (epoch = 2721, loss = 0.0131)\n",
      "Saving model (epoch = 2728, loss = 0.0131)\n",
      "Saving model (epoch = 2729, loss = 0.0131)\n",
      "Saving model (epoch = 2731, loss = 0.0131)\n",
      "Saving model (epoch = 2732, loss = 0.0131)\n",
      "Saving model (epoch = 2734, loss = 0.0131)\n",
      "Saving model (epoch = 2736, loss = 0.0131)\n",
      "Saving model (epoch = 2737, loss = 0.0131)\n",
      "Saving model (epoch = 2742, loss = 0.0131)\n",
      "Saving model (epoch = 2747, loss = 0.0131)\n",
      "Saving model (epoch = 2748, loss = 0.0130)\n",
      "Saving model (epoch = 2750, loss = 0.0130)\n",
      "Saving model (epoch = 2757, loss = 0.0130)\n",
      "Saving model (epoch = 2758, loss = 0.0130)\n",
      "Saving model (epoch = 2764, loss = 0.0130)\n",
      "Saving model (epoch = 2766, loss = 0.0130)\n",
      "Saving model (epoch = 2767, loss = 0.0130)\n",
      "Saving model (epoch = 2770, loss = 0.0130)\n",
      "Saving model (epoch = 2773, loss = 0.0130)\n",
      "Saving model (epoch = 2775, loss = 0.0130)\n",
      "Saving model (epoch = 2778, loss = 0.0130)\n",
      "Saving model (epoch = 2779, loss = 0.0130)\n",
      "Saving model (epoch = 2782, loss = 0.0130)\n",
      "Saving model (epoch = 2784, loss = 0.0130)\n",
      "Saving model (epoch = 2785, loss = 0.0130)\n",
      "Saving model (epoch = 2791, loss = 0.0130)\n",
      "Saving model (epoch = 2792, loss = 0.0130)\n",
      "Saving model (epoch = 2794, loss = 0.0130)\n",
      "Saving model (epoch = 2804, loss = 0.0130)\n",
      "Saving model (epoch = 2805, loss = 0.0130)\n",
      "Saving model (epoch = 2806, loss = 0.0130)\n",
      "Saving model (epoch = 2807, loss = 0.0130)\n",
      "Saving model (epoch = 2811, loss = 0.0130)\n",
      "Saving model (epoch = 2812, loss = 0.0130)\n",
      "Saving model (epoch = 2815, loss = 0.0130)\n",
      "Saving model (epoch = 2817, loss = 0.0130)\n",
      "Saving model (epoch = 2822, loss = 0.0130)\n",
      "Saving model (epoch = 2823, loss = 0.0130)\n",
      "Saving model (epoch = 2829, loss = 0.0130)\n",
      "Saving model (epoch = 2832, loss = 0.0130)\n",
      "Saving model (epoch = 2833, loss = 0.0130)\n",
      "Saving model (epoch = 2838, loss = 0.0130)\n",
      "Saving model (epoch = 2839, loss = 0.0130)\n",
      "Saving model (epoch = 2840, loss = 0.0130)\n",
      "Saving model (epoch = 2841, loss = 0.0130)\n",
      "Saving model (epoch = 2843, loss = 0.0130)\n",
      "Saving model (epoch = 2847, loss = 0.0130)\n",
      "Saving model (epoch = 2855, loss = 0.0130)\n",
      "Saving model (epoch = 2857, loss = 0.0130)\n",
      "Saving model (epoch = 2859, loss = 0.0130)\n",
      "Saving model (epoch = 2860, loss = 0.0130)\n",
      "Saving model (epoch = 2861, loss = 0.0130)\n",
      "Saving model (epoch = 2865, loss = 0.0130)\n",
      "Saving model (epoch = 2866, loss = 0.0130)\n",
      "Saving model (epoch = 2872, loss = 0.0130)\n",
      "Saving model (epoch = 2875, loss = 0.0129)\n",
      "Saving model (epoch = 2881, loss = 0.0129)\n",
      "Saving model (epoch = 2882, loss = 0.0129)\n",
      "Saving model (epoch = 2888, loss = 0.0129)\n",
      "Saving model (epoch = 2889, loss = 0.0129)\n",
      "Saving model (epoch = 2892, loss = 0.0129)\n",
      "Saving model (epoch = 2894, loss = 0.0129)\n",
      "Saving model (epoch = 2895, loss = 0.0129)\n",
      "Saving model (epoch = 2896, loss = 0.0129)\n",
      "Saving model (epoch = 2899, loss = 0.0129)\n",
      "Saving model (epoch = 2903, loss = 0.0129)\n",
      "Saving model (epoch = 2907, loss = 0.0129)\n",
      "Saving model (epoch = 2908, loss = 0.0129)\n",
      "Saving model (epoch = 2909, loss = 0.0129)\n",
      "Saving model (epoch = 2914, loss = 0.0129)\n",
      "Saving model (epoch = 2917, loss = 0.0129)\n",
      "Saving model (epoch = 2918, loss = 0.0129)\n",
      "Saving model (epoch = 2919, loss = 0.0129)\n",
      "Saving model (epoch = 2921, loss = 0.0129)\n",
      "Saving model (epoch = 2925, loss = 0.0129)\n",
      "Saving model (epoch = 2927, loss = 0.0129)\n",
      "Saving model (epoch = 2928, loss = 0.0129)\n",
      "Saving model (epoch = 2930, loss = 0.0129)\n",
      "Saving model (epoch = 2936, loss = 0.0129)\n",
      "Saving model (epoch = 2943, loss = 0.0129)\n",
      "Saving model (epoch = 2944, loss = 0.0129)\n",
      "Saving model (epoch = 2946, loss = 0.0129)\n",
      "Saving model (epoch = 2948, loss = 0.0129)\n",
      "Saving model (epoch = 2952, loss = 0.0129)\n",
      "Saving model (epoch = 2953, loss = 0.0129)\n",
      "Saving model (epoch = 2954, loss = 0.0129)\n",
      "Saving model (epoch = 2955, loss = 0.0129)\n",
      "Saving model (epoch = 2957, loss = 0.0129)\n",
      "Saving model (epoch = 2960, loss = 0.0129)\n",
      "Saving model (epoch = 2965, loss = 0.0129)\n",
      "Saving model (epoch = 2967, loss = 0.0129)\n",
      "Saving model (epoch = 2971, loss = 0.0129)\n",
      "Saving model (epoch = 2973, loss = 0.0129)\n",
      "Saving model (epoch = 2975, loss = 0.0129)\n",
      "Saving model (epoch = 2976, loss = 0.0129)\n",
      "Saving model (epoch = 2979, loss = 0.0129)\n",
      "Saving model (epoch = 2982, loss = 0.0129)\n",
      "Saving model (epoch = 2983, loss = 0.0129)\n",
      "Saving model (epoch = 2987, loss = 0.0129)\n",
      "Saving model (epoch = 2988, loss = 0.0129)\n",
      "Saving model (epoch = 2992, loss = 0.0129)\n",
      "Saving model (epoch = 2995, loss = 0.0129)\n",
      "Saving model (epoch = 2996, loss = 0.0129)\n",
      "Saving model (epoch = 2997, loss = 0.0129)\n",
      "Finished training after 3000 epochs\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(model_loss_record, title='deep model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model\n",
    "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
    "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
    "model.load_state_dict(ckpt)\n",
    "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to pred.csv\n"
     ]
    }
   ],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "\n",
    "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
    "save_pred(preds, 'pred.csv')  # save prediction file to pred.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n0    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n2    4    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n3    1    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n4    9    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   779  780  781  782  783  784  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>775</th>\n      <th>776</th>\n      <th>777</th>\n      <th>778</th>\n      <th>779</th>\n      <th>780</th>\n      <th>781</th>\n      <th>782</th>\n      <th>783</th>\n      <th>784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tr = pd.read_csv('../data/mnist_train.csv', header=None)\n",
    "tr.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "0        5\n1        0\n2        4\n3        1\n4        9\n        ..\n59995    8\n59996    3\n59997    5\n59998    6\n59999    8\nName: 0, Length: 60000, dtype: int64"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.zeros(10)\n",
    "a[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}